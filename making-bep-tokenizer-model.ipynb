{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/golammostofas/making-bep-tokenizer-model?scriptVersionId=168958480\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"529e1afa","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-27T07:28:50.366907Z","iopub.status.busy":"2024-03-27T07:28:50.36645Z","iopub.status.idle":"2024-03-27T07:29:03.64628Z","shell.execute_reply":"2024-03-27T07:29:03.645395Z"},"papermill":{"duration":13.287617,"end_time":"2024-03-27T07:29:03.648906","exception":false,"start_time":"2024-03-27T07:28:50.361289","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\r\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.21.4)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\r\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.3.0)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\r\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\r\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\r\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\r\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (21.3)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.1.1)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\r\n"]}],"source":["!pip install tokenizers\n"]},{"cell_type":"code","execution_count":2,"id":"76371e13","metadata":{"execution":{"iopub.execute_input":"2024-03-27T07:29:03.656722Z","iopub.status.busy":"2024-03-27T07:29:03.656312Z","iopub.status.idle":"2024-03-27T07:29:03.663294Z","shell.execute_reply":"2024-03-27T07:29:03.662325Z"},"papermill":{"duration":0.013482,"end_time":"2024-03-27T07:29:03.665612","exception":false,"start_time":"2024-03-27T07:29:03.65213","status":"completed"},"tags":[]},"outputs":[],"source":["class CustomDecoder:\n","    def __init__(self):\n","        pass\n","\n","    def decode(self, tokens):\n","        # Step 1: Replace \"▁\" with a space\n","        decoded = ' '.join(tokens).replace(\"▁\", \" \")\n","        \n","        # Step 2: ByteFallback (simplified example)\n","        # In a real scenario, you'd check if tokens are out of vocabulary and then decode them byte by byte.\n","        # This is a placeholder to illustrate where such logic would go.\n","        decoded = self.byte_fallback(decoded)\n","        \n","        # Step 3: Fuse (this example does not implement fusing logic as it's highly specific)\n","        # decoded = self.fuse(decoded)\n","        \n","        # Step 4: Strip leading spaces\n","        decoded = decoded.lstrip()\n","        \n","        return decoded\n","    \n","    def byte_fallback(self, text):\n","        # Placeholder for byte fallback logic\n","        # In a real scenario, you'd convert out-of-vocabulary tokens to their byte representation.\n","        return text"]},{"cell_type":"code","execution_count":3,"id":"6f4ad1de","metadata":{"execution":{"iopub.execute_input":"2024-03-27T07:29:03.673346Z","iopub.status.busy":"2024-03-27T07:29:03.672332Z","iopub.status.idle":"2024-03-27T07:29:04.873573Z","shell.execute_reply":"2024-03-27T07:29:04.872596Z"},"papermill":{"duration":1.207362,"end_time":"2024-03-27T07:29:04.875793","exception":false,"start_time":"2024-03-27T07:29:03.668431","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, processors, trainers\n","\n","# Initialize the tokenizer with a BPE model\n","tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","# Normalization: Replacing spaces with a specified character\n","normalizer = normalizers.Sequence([\n","    normalizers.Replace(\" \", \"_\"),\n","    normalizers.Prepend(\"_\")\n","])\n","tokenizer.normalizer = normalizer\n","\n","# Since you mentioned \"pre_tokenizer\": null, we won't set a pre-tokenizer\n","# Note: This might not be practical for most applications\n","\n","# Post-Processor: Adding special tokens around sequences\n","tokenizer.post_processor = processors.TemplateProcessing(\n","    single=\"[CLS] $A [SEP]\",\n","    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n","    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",")\n","\n","# Assuming the \"decoder\": A custom decoding process isn't directly implemented here\n","tokenizer.decoder = decoders.Sequence([\n","    decoders.Replace(\"_\", \" \"),  \n","    decoders.ByteFallback(),     # Fallback to byte encoding for unknown tokens\n","    decoders.Fuse(),             # Fuse tokens - specifics depend on implementation\n","    # Use Strip without 'start', or correctly according to its API.\n","    # This is speculative; you must replace it with the correct usage:\n","    decoders.Strip()  # Assuming Strip is to remove spaces or specific characters from tokens\n","])\n","\n","# Initialize the trainer with special tokens\n","trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n","\n","# Specify paths to your training files\n","files = [\"/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt\"]\n","\n","# Train the tokenizer\n","tokenizer.train(files, trainer)\n","\n","# Save the tokenizer for later use\n","tokenizer.save(\"custom_tokenizer.json\")\n"]},{"cell_type":"code","execution_count":4,"id":"bcea650a","metadata":{"execution":{"iopub.execute_input":"2024-03-27T07:29:04.883265Z","iopub.status.busy":"2024-03-27T07:29:04.88243Z","iopub.status.idle":"2024-03-27T07:29:04.961965Z","shell.execute_reply":"2024-03-27T07:29:04.960503Z"},"papermill":{"duration":0.085391,"end_time":"2024-03-27T07:29:04.964103","exception":false,"start_time":"2024-03-27T07:29:04.878712","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', '_Here_is_', 'some_', 't', 'ex', 't_to_', 'en', 'co', 'de_', '[SEP]']\n","[1, 3855, 586, 76, 257, 1926, 100, 844, 807, 2]\n"]}],"source":["from tokenizers import Tokenizer\n","\n","# Load the tokenizer\n","tokenizer = Tokenizer.from_file(\"/kaggle/working/custom_tokenizer.json\")\n","\n","# Encode some text\n","encoded = tokenizer.encode(\"Here is some text to encode_\")\n","\n","# Print the tokens and IDs\n","print(encoded.tokens)\n","print(encoded.ids)\n"]},{"cell_type":"code","execution_count":5,"id":"94e2c350","metadata":{"execution":{"iopub.execute_input":"2024-03-27T07:29:04.971262Z","iopub.status.busy":"2024-03-27T07:29:04.97093Z","iopub.status.idle":"2024-03-27T07:29:05.047625Z","shell.execute_reply":"2024-03-27T07:29:05.04612Z"},"papermill":{"duration":0.082895,"end_time":"2024-03-27T07:29:05.049906","exception":false,"start_time":"2024-03-27T07:29:04.967011","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" Here is some text to encode \n"]}],"source":["# Assuming you've already loaded your tokenizer as shown previously\n","from tokenizers import Tokenizer\n","\n","tokenizer = Tokenizer.from_file(\"/kaggle/working/custom_tokenizer.json\")\n","\n","\n","# Decode the token IDs\n","decoded_text = tokenizer.decode(encoded.ids)\n","\n","print(decoded_text)\n"]},{"cell_type":"code","execution_count":null,"id":"f27ad4a7","metadata":{"papermill":{"duration":0.00283,"end_time":"2024-03-27T07:29:05.056027","exception":false,"start_time":"2024-03-27T07:29:05.053197","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4640957,"sourceId":7902037,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":17.835168,"end_time":"2024-03-27T07:29:05.479189","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-27T07:28:47.644021","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}