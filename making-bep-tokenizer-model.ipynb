{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/golammostofas/making-bep-tokenizer-model?scriptVersionId=168947622\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"cf5a4fbd","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-27T06:02:09.249175Z","iopub.status.busy":"2024-03-27T06:02:09.248481Z","iopub.status.idle":"2024-03-27T06:02:26.899918Z","shell.execute_reply":"2024-03-27T06:02:26.898472Z"},"papermill":{"duration":17.660241,"end_time":"2024-03-27T06:02:26.903089","exception":false,"start_time":"2024-03-27T06:02:09.242848","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\r\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.21.4)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\r\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.3.0)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\r\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\r\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\r\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\r\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (21.3)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.1.1)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\r\n"]}],"source":["!pip install tokenizers\n"]},{"cell_type":"code","execution_count":null,"id":"28c7f6ed","metadata":{"papermill":{"duration":0.003417,"end_time":"2024-03-27T06:02:26.910504","exception":false,"start_time":"2024-03-27T06:02:26.907087","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"id":"f933d642","metadata":{"execution":{"iopub.execute_input":"2024-03-27T06:02:26.921263Z","iopub.status.busy":"2024-03-27T06:02:26.919884Z","iopub.status.idle":"2024-03-27T06:02:28.399994Z","shell.execute_reply":"2024-03-27T06:02:28.398592Z"},"papermill":{"duration":1.489015,"end_time":"2024-03-27T06:02:28.403465","exception":false,"start_time":"2024-03-27T06:02:26.91445","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, processors, trainers\n","\n","# Initialize the tokenizer with a BPE model\n","tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","# Normalization: Replacing spaces with a specified character\n","normalizer = normalizers.Sequence([\n","    normalizers.Replace(\" \", \"_\")\n","])\n","tokenizer.normalizer = normalizer\n","\n","# Since you mentioned \"pre_tokenizer\": null, we won't set a pre-tokenizer\n","# Note: This might not be practical for most applications\n","\n","# Post-Processor: Adding special tokens around sequences\n","tokenizer.post_processor = processors.TemplateProcessing(\n","    single=\"[CLS] $A [SEP]\",\n","    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n","    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",")\n","\n","# Assuming the \"decoder\": A custom decoding process isn't directly implemented here\n","# A fallback to using the standard decoder for BPE\n","tokenizer.decoder = decoders.BPEDecoder()\n","\n","# Initialize the trainer with special tokens\n","trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n","\n","# Specify paths to your training files\n","files = [\"/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt\"]\n","\n","# Train the tokenizer\n","tokenizer.train(files, trainer)\n","\n","# Save the tokenizer for later use\n","tokenizer.save(\"custom_tokenizer.json\")\n"]},{"cell_type":"code","execution_count":3,"id":"9ad9b8e2","metadata":{"execution":{"iopub.execute_input":"2024-03-27T06:02:28.417116Z","iopub.status.busy":"2024-03-27T06:02:28.416448Z","iopub.status.idle":"2024-03-27T06:02:28.498696Z","shell.execute_reply":"2024-03-27T06:02:28.497737Z"},"papermill":{"duration":0.093059,"end_time":"2024-03-27T06:02:28.502045","exception":false,"start_time":"2024-03-27T06:02:28.408986","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', 'Here_', 'is_', 'some_', 'tex', 't_to_', 'en', 'co', 'de_', '[SEP]']\n","[1, 831, 104, 588, 7525, 2210, 99, 608, 765, 2]\n"]}],"source":["from tokenizers import Tokenizer\n","\n","# Load the tokenizer\n","tokenizer = Tokenizer.from_file(\"/kaggle/working/custom_tokenizer.json\")\n","\n","# Encode some text\n","encoded = tokenizer.encode(\"Here is some text to encode_\")\n","\n","# Print the tokens and IDs\n","print(encoded.tokens)\n","print(encoded.ids)\n"]},{"cell_type":"code","execution_count":4,"id":"eaec5e44","metadata":{"execution":{"iopub.execute_input":"2024-03-27T06:02:28.515733Z","iopub.status.busy":"2024-03-27T06:02:28.515298Z","iopub.status.idle":"2024-03-27T06:02:28.592999Z","shell.execute_reply":"2024-03-27T06:02:28.591741Z"},"papermill":{"duration":0.087847,"end_time":"2024-03-27T06:02:28.596041","exception":false,"start_time":"2024-03-27T06:02:28.508194","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Here is some text to encode \n"]}],"source":["# Assuming you've already loaded your tokenizer as shown previously\n","from tokenizers import Tokenizer\n","\n","tokenizer = Tokenizer.from_file(\"/kaggle/working/custom_tokenizer.json\")\n","\n","\n","# Decode the token IDs\n","decoded_text = tokenizer.decode(encoded.ids)\n","# Replace underscores with spaces in the decoded text\n","decoded_text_with_spaces = decoded_text.replace(\"_\", \" \")\n","print(decoded_text_with_spaces)\n"]},{"cell_type":"code","execution_count":null,"id":"5000f15c","metadata":{"papermill":{"duration":0.005284,"end_time":"2024-03-27T06:02:28.60698","exception":false,"start_time":"2024-03-27T06:02:28.601696","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4640957,"sourceId":7902037,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":23.573282,"end_time":"2024-03-27T06:02:29.033418","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-27T06:02:05.460136","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}