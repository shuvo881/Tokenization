{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/golammostofas/depth-knowledge-on-tokenization?scriptVersionId=167964609\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":null,"id":"51814d52","metadata":{"_cell_guid":"8d01a664-aa16-4771-8581-b9fba3abc318","_uuid":"7a05e32e-5094-4d77-b8e2-0ddf6bd2d46d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.008044,"end_time":"2024-03-20T11:12:22.17519","exception":false,"start_time":"2024-03-20T11:12:22.167146","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"e03c612e","metadata":{"papermill":{"duration":0.007271,"end_time":"2024-03-20T11:12:22.190206","exception":false,"start_time":"2024-03-20T11:12:22.182935","status":"completed"},"tags":[]},"source":[]},{"cell_type":"markdown","id":"c1c782e0","metadata":{"_cell_guid":"750482ac-b30a-40be-b2a0-54978addee61","_uuid":"a38c4952-764d-4b9c-8e18-52f1184b6861","papermill":{"duration":0.007462,"end_time":"2024-03-20T11:12:22.205038","exception":false,"start_time":"2024-03-20T11:12:22.197576","status":"completed"},"tags":[]},"source":["# Abstract\n","Tokenization is the process of converting a sequence of text into individual units, commonly known as ‘token’. In NLP context, tokens can represent word, subword, or even characters. \n","The primary goal is to prepare raw text data into a format that computational models can more easily analyze.\n","\n","## Role in Large Language Models(LLMs):\n","1. **Traning Phase:**  Data Preprocessing, Sequince Alignments\n","2. **Inference Phase:** Query Understanding, Output Generation"]},{"cell_type":"markdown","id":"308cecc0","metadata":{"_cell_guid":"33dbcd17-d887-4c14-bc6f-7c13c755a1b2","_uuid":"dfbabc75-bbf7-4f03-b1c3-ddb38660e108","papermill":{"duration":0.007221,"end_time":"2024-03-20T11:12:22.219689","exception":false,"start_time":"2024-03-20T11:12:22.212468","status":"completed"},"tags":[]},"source":["## Type of Tokenization\n","\n","1. Word Tokenization\n","2. Subword Tokenization\n","3. Character Tokenization\n","4. Morphological Tokenizaton"]},{"cell_type":"markdown","id":"3943884b","metadata":{"_cell_guid":"7bf9abf5-c0fa-4dac-a98f-49abe1ee80ed","_uuid":"f238c28c-799f-417f-86dd-f0ac1cca4b4f","papermill":{"duration":0.007074,"end_time":"2024-03-20T11:12:22.234104","exception":false,"start_time":"2024-03-20T11:12:22.22703","status":"completed"},"tags":[]},"source":["### 1. Word Tokenization:\n","\n","word Tokenization is one of the earliest and simplest forms of text segmentation. It generally involves splitting a sequence of text into individual word.\n","\n","2 type common algorithm:\n","1. Whitespace Tokenization\n","2. Rule-Based Tokenization"]},{"cell_type":"markdown","id":"45c1c44e","metadata":{"_cell_guid":"abd5f38a-d555-41b0-9ddf-269519d536ef","_uuid":"02318060-088c-4c4e-9e21-ee88d6c3c675","papermill":{"duration":0.007093,"end_time":"2024-03-20T11:12:22.248574","exception":false,"start_time":"2024-03-20T11:12:22.241481","status":"completed"},"tags":[]},"source":["#### 1.1. Whitespace Tokenization\n","The most basic from of word tokenization is whitespace tokenization, whitch splits text based on spaces."]},{"cell_type":"code","execution_count":1,"id":"d0dcd4b6","metadata":{"_cell_guid":"a660a78a-4500-4225-91e0-5bade051428c","_uuid":"a7090380-4e63-45bb-b47f-49329c078aac","collapsed":false,"execution":{"iopub.execute_input":"2024-03-20T11:12:22.264416Z","iopub.status.busy":"2024-03-20T11:12:22.264105Z","iopub.status.idle":"2024-03-20T11:12:22.276046Z","shell.execute_reply":"2024-03-20T11:12:22.275104Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022031,"end_time":"2024-03-20T11:12:22.277942","exception":false,"start_time":"2024-03-20T11:12:22.255911","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'whitespace', 'tokenization']\n"]}],"source":["# code of whitespace tokenization\n","\n","def whitespace_tokenization(text):\n","    return text.split()\n","\n","text = 'Hello, this is an example text to demonstrate whitespace tokenization'\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"6c4d2d4e","metadata":{"_cell_guid":"e8bfcdfe-c0fc-4c1c-8a32-c024a70e66b5","_uuid":"036ce52a-b68b-4b91-b975-eb5590c57dff","papermill":{"duration":0.007277,"end_time":"2024-03-20T11:12:22.292676","exception":false,"start_time":"2024-03-20T11:12:22.285399","status":"completed"},"tags":[]},"source":["#### 1.2. Rule-Base Tokenization\n","This approach used a set of predefined rules and regex patterns to identify tokens. Example:"]},{"cell_type":"code","execution_count":2,"id":"a2b29a30","metadata":{"_cell_guid":"913c6bc5-6edb-47d7-b9ca-12c38e24e9c1","_uuid":"2567987c-2dc1-4519-8f35-f020aaccef2a","collapsed":false,"execution":{"iopub.execute_input":"2024-03-20T11:12:22.309252Z","iopub.status.busy":"2024-03-20T11:12:22.30898Z","iopub.status.idle":"2024-03-20T11:12:22.314808Z","shell.execute_reply":"2024-03-20T11:12:22.313937Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015936,"end_time":"2024-03-20T11:12:22.31676","exception":false,"start_time":"2024-03-20T11:12:22.300824","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# code of rule-base tokenization:\n","\n","import re\n","\n","\n","def rule_base_tokenization(text):\n","    # define regex pattern\n","    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n","    \n","    tokens = re.findall(pattern, text)\n","    \n","    return tokens\n","\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"b09fbb0f","metadata":{"_cell_guid":"aee8ea5d-9b2b-462e-936c-fa294a22fc7e","_uuid":"105359d5-f858-4c77-8153-f09fb47c1d11","papermill":{"duration":0.007418,"end_time":"2024-03-20T11:12:22.331734","exception":false,"start_time":"2024-03-20T11:12:22.324316","status":"completed"},"tags":[]},"source":["##### comparison between whitespace and rule-based tokenization"]},{"cell_type":"code","execution_count":3,"id":"deaed3be","metadata":{"_cell_guid":"c1b62537-e848-449a-a1a9-89f20ff79503","_uuid":"ac750ca1-c884-44db-be87-8a0c613e8fe6","collapsed":false,"execution":{"iopub.execute_input":"2024-03-20T11:12:22.347661Z","iopub.status.busy":"2024-03-20T11:12:22.347372Z","iopub.status.idle":"2024-03-20T11:12:22.352261Z","shell.execute_reply":"2024-03-20T11:12:22.351384Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015002,"end_time":"2024-03-20T11:12:22.354214","exception":false,"start_time":"2024-03-20T11:12:22.339212","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens by Whitespace Tokenization:\n"," ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule-based', 'tokenization!', \"Isn't\", 'it', 'great?']\n","Tokens by Rule-base Tokenization:\n"," ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# used common text\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens by Whitespace Tokenization:\\n', tokens)\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens by Rule-base Tokenization:\\n', tokens)"]},{"cell_type":"markdown","id":"81d73895","metadata":{"_cell_guid":"74110206-112e-4349-8901-f9660fdcf05f","_uuid":"e6d0409d-ddc1-4139-bfeb-dd3490fb38b3","papermill":{"duration":0.00743,"end_time":"2024-03-20T11:12:22.369144","exception":false,"start_time":"2024-03-20T11:12:22.361714","status":"completed"},"tags":[]},"source":["### 2. Subword Tokenization\n","\n","Subword Tokenization techniques operate at a lavel between words and characterss, aiming to capture meaningful linguistic units smaller then a word but large then a character\n","\n","3 common algorithms:\n","\n","1. Byte Pair Encoding(BPE)\n","2. WordPiece \n","3. SentencePiece"]},{"cell_type":"markdown","id":"1e57c352","metadata":{"_cell_guid":"5ea3227f-8222-4378-9ab9-3fe05da4ea70","_uuid":"89acf4dc-319f-49d8-a4f2-fdd9d510784d","papermill":{"duration":0.007296,"end_time":"2024-03-20T11:12:22.384371","exception":false,"start_time":"2024-03-20T11:12:22.377075","status":"completed"},"tags":[]},"source":["#### 2.1. Byte Pair Encoding(BPE)\n","\n","BPE works by iteratively merginf the most frequently occurring character or character sequences. Following a somplified illustration of how BPE works tokenizing text:\n","* **Initialization:** Start with individual characters or symbols as the basic tokens\n","* **Frequency Count:** Count all adjancent pairs of tokens in the dataset\n","* **Merge:** Identifiy the most frequent pair of tokens and merge them into a single new token\n","* **Repeat:** Repeat the frequency count and merge steps untill a sepecified number of merges has been reached or the vocabulary reaches a desired size\n","\n","Example:\n","\n","Suppose the data to be encoded is\n","> aaabdaaabac\n","\n","The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:\n","> ZabdZabac\n","\n","> Z=aa\n","\n","Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n","> ZYdZYac\n","\n","> Y=ab\n","\n","> Z=aa\n","\n","The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n","> XdXac\n","\n","> X=ZY\n","\n","> Y=ab\n","\n","> Z=aa\n","\n","*This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.*\n","\n","**N.B:** To decompress the data, simply perform the replacements in the reverse order.\n","Below is a basic python implementation of BPE:"]},{"cell_type":"code","execution_count":4,"id":"db24a186","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.400483Z","iopub.status.busy":"2024-03-20T11:12:22.400224Z","iopub.status.idle":"2024-03-20T11:12:22.408057Z","shell.execute_reply":"2024-03-20T11:12:22.407223Z"},"papermill":{"duration":0.017937,"end_time":"2024-03-20T11:12:22.409892","exception":false,"start_time":"2024-03-20T11:12:22.391955","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'this': 1, 'is': 1, 'an': 2, 'example.': 1, 'I': 1, 'am': 1, 'engineer': 1}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from collections import defaultdict, Counter\n","\n","def get_vocab(text):\n","    \"\"\"Split text into symbles and connt symbols frequencies\"\"\"\n","    \n","    vocab = Counter(text.split())\n","    \n","    #convert vocabulary to format {'word': count}\n","    \n","    return {word: freq for word, freq in vocab.items()}\n","\n","get_vocab('this is an example. I am an engineer')\n","    "]},{"cell_type":"code","execution_count":5,"id":"ffaf5543","metadata":{"_cell_guid":"894656c7-c18c-40be-b1cf-4abf83b43c4f","_uuid":"7b3b553c-5c7a-429e-81b8-30a7fda47819","collapsed":false,"execution":{"iopub.execute_input":"2024-03-20T11:12:22.426725Z","iopub.status.busy":"2024-03-20T11:12:22.426457Z","iopub.status.idle":"2024-03-20T11:12:22.433454Z","shell.execute_reply":"2024-03-20T11:12:22.43264Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.017712,"end_time":"2024-03-20T11:12:22.435454","exception":false,"start_time":"2024-03-20T11:12:22.417742","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["defaultdict(int, {})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","def get_stats(vocab):\n","    \n","    pairs = defaultdict(int)\n","\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i  in range(len(symbols) - 1):\n","            pairs[symbols[i], symbols[i+1]] += freq\n","    return pairs\n","\n","vocab = get_vocab('this is an example. I am an engineer.')\n","get_stats(vocab)"]},{"cell_type":"code","execution_count":6,"id":"0b099598","metadata":{"_cell_guid":"79b8a12c-42e0-4db7-a701-a1a8eb1d3d65","_uuid":"430c4da5-4e20-47f6-aa81-ebcf754b7dc7","collapsed":false,"execution":{"iopub.execute_input":"2024-03-20T11:12:22.452152Z","iopub.status.busy":"2024-03-20T11:12:22.451908Z","iopub.status.idle":"2024-03-20T11:12:22.456742Z","shell.execute_reply":"2024-03-20T11:12:22.455923Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015253,"end_time":"2024-03-20T11:12:22.458567","exception":false,"start_time":"2024-03-20T11:12:22.443314","status":"completed"},"tags":[]},"outputs":[],"source":["def marge_vocab(pair, vocab):\n","    new_vocab = {}\n","    bigram = ' '.join(pair)\n","    replacement = ''.join(pair)\n","    for word in vocab:\n","        new_word = word.replace(bigram, replacement)\n","        new_vocab[new_word] = vocab[word]\n","        \n","    return new_vocab\n","\n"]},{"cell_type":"code","execution_count":7,"id":"e3e8b162","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.47545Z","iopub.status.busy":"2024-03-20T11:12:22.47522Z","iopub.status.idle":"2024-03-20T11:12:22.483116Z","shell.execute_reply":"2024-03-20T11:12:22.482159Z"},"papermill":{"duration":0.018379,"end_time":"2024-03-20T11:12:22.485046","exception":false,"start_time":"2024-03-20T11:12:22.466667","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'I', 'am', 'an', 'engineer.', 'example.', 'is', 'test', 'this'}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["def bpe_tokenize(text, number_merges):\n","    vocab = get_vocab(text)\n","    for i in range(number_merges):\n","        pairs = get_stats(vocab)\n","        if not pairs:\n","            break\n","        best = max(pairs, key=pairs.get)\n","        vocab = marge_vocab(best, vocab)\n","        \n","    tokens = set()\n","    for word in vocab:\n","        tokens.update(word.split())\n","        \n","    return tokens\n","\n","text = 'this is an example. I am an engineer. this is test'\n","\n","tokens = bpe_tokenize(text, 10)\n","tokens"]},{"cell_type":"markdown","id":"8a2bb170","metadata":{"papermill":{"duration":0.007904,"end_time":"2024-03-20T11:12:22.500894","exception":false,"start_time":"2024-03-20T11:12:22.49299","status":"completed"},"tags":[]},"source":["##### For LLMs"]},{"cell_type":"code","execution_count":8,"id":"dbd88f2a","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.518157Z","iopub.status.busy":"2024-03-20T11:12:22.517915Z","iopub.status.idle":"2024-03-20T11:12:22.523364Z","shell.execute_reply":"2024-03-20T11:12:22.522559Z"},"papermill":{"duration":0.016206,"end_time":"2024-03-20T11:12:22.525313","exception":false,"start_time":"2024-03-20T11:12:22.509107","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[116, 104, 105, 115, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 73, 32, 97, 109, 32, 97, 110, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46, 32, 116, 104, 105, 115, 32, 105, 115, 32, 116, 101, 115, 116]\n","\n","len of text 50\n","len of token 50\n"]}],"source":["def encoding(text):\n","    tokens = text.encode('utf-8') # Raw bytes\n","    return list(map(int, tokens))\n","\n","text = 'this is an example. I am an engineer. this is test'\n","tokens = encoding(text)\n","print(tokens)\n","print('\\nlen of text', len(text))\n","print('len of token', len(tokens))\n","\n"]},{"cell_type":"code","execution_count":9,"id":"b38b502a","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.542599Z","iopub.status.busy":"2024-03-20T11:12:22.542336Z","iopub.status.idle":"2024-03-20T11:12:22.547722Z","shell.execute_reply":"2024-03-20T11:12:22.54686Z"},"papermill":{"duration":0.016419,"end_time":"2024-03-20T11:12:22.549724","exception":false,"start_time":"2024-03-20T11:12:22.533305","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{(116, 104): 2, (104, 105): 2, (105, 115): 4, (115, 32): 4, (32, 105): 2, (32, 97): 3, (97, 110): 2, (110, 32): 2, (32, 101): 2, (101, 120): 1, (120, 97): 1, (97, 109): 2, (109, 112): 1, (112, 108): 1, (108, 101): 1, (101, 46): 1, (46, 32): 2, (32, 73): 1, (73, 32): 1, (109, 32): 1, (101, 110): 1, (110, 103): 1, (103, 105): 1, (105, 110): 1, (110, 101): 1, (101, 101): 1, (101, 114): 1, (114, 46): 1, (32, 116): 2, (116, 101): 1, (101, 115): 1, (115, 116): 1}\n"]}],"source":["def get_stats(ids):\n","    counts = {}\n","    for pair in zip(ids, ids[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","    return counts\n","\n","stats = get_stats(tokens)\n","\n","print(stats)"]},{"cell_type":"code","execution_count":10,"id":"a377dac5","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.567065Z","iopub.status.busy":"2024-03-20T11:12:22.566832Z","iopub.status.idle":"2024-03-20T11:12:22.572441Z","shell.execute_reply":"2024-03-20T11:12:22.571606Z"},"papermill":{"duration":0.016438,"end_time":"2024-03-20T11:12:22.5744","exception":false,"start_time":"2024-03-20T11:12:22.557962","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(105, 115)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["top_pair = max(stats, key=stats.get)\n","top_pair"]},{"cell_type":"code","execution_count":11,"id":"a44ad9e1","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.592305Z","iopub.status.busy":"2024-03-20T11:12:22.592072Z","iopub.status.idle":"2024-03-20T11:12:22.597854Z","shell.execute_reply":"2024-03-20T11:12:22.596836Z"},"papermill":{"duration":0.016714,"end_time":"2024-03-20T11:12:22.59981","exception":false,"start_time":"2024-03-20T11:12:22.583096","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["('i', 's')"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["chr(105), chr(115)"]},{"cell_type":"code","execution_count":12,"id":"d8599cf6","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.619316Z","iopub.status.busy":"2024-03-20T11:12:22.619028Z","iopub.status.idle":"2024-03-20T11:12:22.627501Z","shell.execute_reply":"2024-03-20T11:12:22.626655Z"},"papermill":{"duration":0.021169,"end_time":"2024-03-20T11:12:22.629523","exception":false,"start_time":"2024-03-20T11:12:22.608354","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[5, 6, 99, 9, 1]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["def merge(ids, pair, idx):\n","    new_ids = []\n","    \n","    i = 0\n","    while i < len(ids):\n","        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n","            new_ids.append(idx)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids\n","\n","merge([5, 6, 6, 7, 9, 1], (6, 7), 99)"]},{"cell_type":"code","execution_count":13,"id":"d3880627","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.650191Z","iopub.status.busy":"2024-03-20T11:12:22.649474Z","iopub.status.idle":"2024-03-20T11:12:22.655197Z","shell.execute_reply":"2024-03-20T11:12:22.653874Z"},"papermill":{"duration":0.01845,"end_time":"2024-03-20T11:12:22.657537","exception":false,"start_time":"2024-03-20T11:12:22.639087","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[116, 104, 256, 32, 256, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 73, 32, 97, 109, 32, 97, 110, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46, 32, 116, 104, 256, 32, 256, 32, 116, 101, 115, 116]\n","\n","len of token2: 46\n"]}],"source":["tokens2 = merge(tokens, top_pair, 256)\n","print(tokens2)\n","\n","print('\\nlen of token2:', len(tokens2))\n"]},{"cell_type":"code","execution_count":14,"id":"f70d5ad8","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.678189Z","iopub.status.busy":"2024-03-20T11:12:22.677934Z","iopub.status.idle":"2024-03-20T11:12:22.684696Z","shell.execute_reply":"2024-03-20T11:12:22.683688Z"},"papermill":{"duration":0.019263,"end_time":"2024-03-20T11:12:22.686982","exception":false,"start_time":"2024-03-20T11:12:22.667719","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Merging: (97, 97) in a new token 256\n","Merging: (256, 97) in a new token 257\n","Merging: (257, 98) in a new token 258\n"]}],"source":["#using constant time\n","vocab_size = 276\n","number_megres = vocab_size - 256\n","text = 'aaabdaaabac'\n","tokens = encoding(text)\n","ids = tokens\n","\n","merges = {}\n","\n","for i in range(number_megres):\n","    stats = get_stats(ids)\n","    \n","    pair = max(stats, key=stats.get)\n","    if stats[pair] == 1:\n","        break\n","    idx = 256 + i\n","    print(f\"Merging: {pair} in a new token {idx}\")\n","    ids = merge(ids, pair, idx)\n","    merges[pair] = idx"]},{"cell_type":"code","execution_count":15,"id":"90f58c92","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.707528Z","iopub.status.busy":"2024-03-20T11:12:22.707285Z","iopub.status.idle":"2024-03-20T11:12:22.712603Z","shell.execute_reply":"2024-03-20T11:12:22.711783Z"},"papermill":{"duration":0.017897,"end_time":"2024-03-20T11:12:22.714738","exception":false,"start_time":"2024-03-20T11:12:22.696841","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{(97, 97): 256, (256, 97): 257, (257, 98): 258}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["merges"]},{"cell_type":"code","execution_count":16,"id":"8597ef48","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.735969Z","iopub.status.busy":"2024-03-20T11:12:22.735675Z","iopub.status.idle":"2024-03-20T11:12:22.741008Z","shell.execute_reply":"2024-03-20T11:12:22.739985Z"},"papermill":{"duration":0.018839,"end_time":"2024-03-20T11:12:22.743273","exception":false,"start_time":"2024-03-20T11:12:22.724434","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["token len 11\n","ids len 5\n","compression ratio:  2.2\n"]}],"source":["print('token len', len(tokens))\n","print('ids len', len(ids))\n","print('compression ratio: ', len(tokens)/len(ids))"]},{"cell_type":"code","execution_count":17,"id":"633404c2","metadata":{"execution":{"iopub.execute_input":"2024-03-20T11:12:22.765236Z","iopub.status.busy":"2024-03-20T11:12:22.764933Z","iopub.status.idle":"2024-03-20T11:12:22.770506Z","shell.execute_reply":"2024-03-20T11:12:22.769605Z"},"papermill":{"duration":0.019025,"end_time":"2024-03-20T11:12:22.772792","exception":false,"start_time":"2024-03-20T11:12:22.753767","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[258, 100, 258, 97, 99]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["ids"]},{"cell_type":"code","execution_count":null,"id":"b0938274","metadata":{"papermill":{"duration":0.010373,"end_time":"2024-03-20T11:12:22.79367","exception":false,"start_time":"2024-03-20T11:12:22.783297","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":4.713038,"end_time":"2024-03-20T11:12:23.021816","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-20T11:12:18.308778","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}