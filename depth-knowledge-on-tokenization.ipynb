{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/golammostofas/depth-knowledge-on-tokenization?scriptVersionId=167755802\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"5c28e556","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-19T07:59:30.033663Z","iopub.status.busy":"2024-03-19T07:59:30.03291Z","iopub.status.idle":"2024-03-19T07:59:30.915424Z","shell.execute_reply":"2024-03-19T07:59:30.914285Z"},"papermill":{"duration":0.892466,"end_time":"2024-03-19T07:59:30.918237","exception":false,"start_time":"2024-03-19T07:59:30.025771","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"a407788f","metadata":{"papermill":{"duration":0.004461,"end_time":"2024-03-19T07:59:30.927873","exception":false,"start_time":"2024-03-19T07:59:30.923412","status":"completed"},"tags":[]},"source":["# Abstract\n","Tokenization is the process of converting a sequence of text into individual units, commonly known as ‘token’. In NLP context, tokens can represent word, subword, or even characters. \n","The primary goal is to prepare raw text data into a format that computational models can more easily analyze.\n","\n","## Role in Large Language Models(LLMs):\n","1. **Traning Phase:**  Data Preprocessing, Sequince Alignments\n","2. **Inference Phase:** Query Understanding, Output Generation\n"]},{"cell_type":"markdown","id":"df996ce7","metadata":{"papermill":{"duration":0.00434,"end_time":"2024-03-19T07:59:30.936918","exception":false,"start_time":"2024-03-19T07:59:30.932578","status":"completed"},"tags":[]},"source":["## Type of Tokenization\n","\n","1. Word Tokenization\n","2. Subword Tokenization\n","3. Character Tokenization\n","4. Morphological Tokenizaton\n"]},{"cell_type":"markdown","id":"636472ae","metadata":{"papermill":{"duration":0.004332,"end_time":"2024-03-19T07:59:30.945827","exception":false,"start_time":"2024-03-19T07:59:30.941495","status":"completed"},"tags":[]},"source":["### 1. Word Tokenization:\n","\n","word Tokenization is one of the earliest and simplest forms of text segmentation. It generally involves splitting a sequence of text into individual word.\n","\n","2 type common algorithm:\n","1. Whitespace Tokenization\n","2. Rule-Based Tokenization\n"]},{"cell_type":"markdown","id":"49add3a7","metadata":{"papermill":{"duration":0.004317,"end_time":"2024-03-19T07:59:30.954665","exception":false,"start_time":"2024-03-19T07:59:30.950348","status":"completed"},"tags":[]},"source":["#### 1.1. Whitespace Tokenization\n","The most basic from of word tokenization is whitespace tokenization, whitch splits text based on spaces."]},{"cell_type":"code","execution_count":2,"id":"c1dc32fd","metadata":{"execution":{"iopub.execute_input":"2024-03-19T07:59:30.96607Z","iopub.status.busy":"2024-03-19T07:59:30.964827Z","iopub.status.idle":"2024-03-19T07:59:30.971667Z","shell.execute_reply":"2024-03-19T07:59:30.970618Z"},"papermill":{"duration":0.015018,"end_time":"2024-03-19T07:59:30.97414","exception":false,"start_time":"2024-03-19T07:59:30.959122","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'whitespace', 'tokenization']\n"]}],"source":["# code of whitespace tokenization\n","\n","def whitespace_tokenization(text):\n","    return text.split()\n","\n","text = 'Hello, this is an example text to demonstrate whitespace tokenization'\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"bd0b56d2","metadata":{"papermill":{"duration":0.004522,"end_time":"2024-03-19T07:59:30.9837","exception":false,"start_time":"2024-03-19T07:59:30.979178","status":"completed"},"tags":[]},"source":["#### 1.2. Rule-Base Tokenization\n","This approach used a set of predefined rules and regex patterns to identify tokens. Example:\n"]},{"cell_type":"code","execution_count":3,"id":"1204a70b","metadata":{"execution":{"iopub.execute_input":"2024-03-19T07:59:30.99557Z","iopub.status.busy":"2024-03-19T07:59:30.994969Z","iopub.status.idle":"2024-03-19T07:59:31.002784Z","shell.execute_reply":"2024-03-19T07:59:31.00146Z"},"papermill":{"duration":0.01649,"end_time":"2024-03-19T07:59:31.005026","exception":false,"start_time":"2024-03-19T07:59:30.988536","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# code of rule-base tokenization:\n","\n","import re\n","\n","\n","def rule_base_tokenization(text):\n","    # define regex pattern\n","    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n","    \n","    tokens = re.findall(pattern, text)\n","    \n","    return tokens\n","\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"5e2465a0","metadata":{"papermill":{"duration":0.004815,"end_time":"2024-03-19T07:59:31.014814","exception":false,"start_time":"2024-03-19T07:59:31.009999","status":"completed"},"tags":[]},"source":["##### comparison between whitespace and rule-based tokenization"]},{"cell_type":"code","execution_count":4,"id":"f6fb27c9","metadata":{"execution":{"iopub.execute_input":"2024-03-19T07:59:31.026869Z","iopub.status.busy":"2024-03-19T07:59:31.02621Z","iopub.status.idle":"2024-03-19T07:59:31.032816Z","shell.execute_reply":"2024-03-19T07:59:31.03168Z"},"papermill":{"duration":0.015189,"end_time":"2024-03-19T07:59:31.034937","exception":false,"start_time":"2024-03-19T07:59:31.019748","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens by Whitespace Tokenization:\n"," ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule-based', 'tokenization!', \"Isn't\", 'it', 'great?']\n","Tokens by Rule-base Tokenization:\n"," ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# used common text\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens by Whitespace Tokenization:\\n', tokens)\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens by Rule-base Tokenization:\\n', tokens)"]},{"cell_type":"markdown","id":"4ad67f2e","metadata":{"papermill":{"duration":0.00503,"end_time":"2024-03-19T07:59:31.044976","exception":false,"start_time":"2024-03-19T07:59:31.039946","status":"completed"},"tags":[]},"source":["### 2. Subword Tokenization\n","\n","Subword Tokenization techniques operate at a lavel between words and characterss, aiming to capture meaningful linguistic units smaller then a word but large then a character\n","\n","3 common algorithms:\n","\n","1. Byte Pair Encoding(BPE)\n","2. WordPiece \n","3. SentencePiece"]},{"cell_type":"markdown","id":"361ec1f4","metadata":{"papermill":{"duration":0.004761,"end_time":"2024-03-19T07:59:31.055127","exception":false,"start_time":"2024-03-19T07:59:31.050366","status":"completed"},"tags":[]},"source":["#### 2.1. Byte Pair Encoding(BPE)\n","\n","BPE works by iteratively merginf the most frequently occurring character or character sequences. Following a somplified illustration of how BPE works tokenizing text:\n","* **Initialization:** Start with individual characters or symbols as the basic tokens\n","* **Frequency Count:** Count all adjancent pairs of tokens in the dataset\n","* **Merge:** Identifiy the most frequent pair of tokens and merge them into a single new token\n","* **Repeat:** Repeat the frequency count and merge steps untill a sepecified number of merges has been reached or the vocabulary reaches a desired size\n","\n","\n","Below is a basic python implementation of BPE:"]},{"cell_type":"code","execution_count":5,"id":"b829b615","metadata":{"execution":{"iopub.execute_input":"2024-03-19T07:59:31.067139Z","iopub.status.busy":"2024-03-19T07:59:31.066462Z","iopub.status.idle":"2024-03-19T07:59:31.073023Z","shell.execute_reply":"2024-03-19T07:59:31.071897Z"},"papermill":{"duration":0.015286,"end_time":"2024-03-19T07:59:31.075407","exception":false,"start_time":"2024-03-19T07:59:31.060121","status":"completed"},"tags":[]},"outputs":[],"source":["from collections import defaultdict, Counter\n","\n","def get_stats(vocab):\n","    \n","    pairs = defaultdicg(int)\n","    \n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i  in range(len(symbols) - 1):\n","            pass"]},{"cell_type":"code","execution_count":null,"id":"10fa63d9","metadata":{"papermill":{"duration":0.004837,"end_time":"2024-03-19T07:59:31.085311","exception":false,"start_time":"2024-03-19T07:59:31.080474","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":5.112315,"end_time":"2024-03-19T07:59:31.610206","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-19T07:59:26.497891","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}