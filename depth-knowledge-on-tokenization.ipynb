{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/golammostofas/depth-knowledge-on-tokenization?scriptVersionId=168964565\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":null,"id":"f6e1b3e2","metadata":{"_cell_guid":"8d01a664-aa16-4771-8581-b9fba3abc318","_uuid":"7a05e32e-5094-4d77-b8e2-0ddf6bd2d46d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.015504,"end_time":"2024-03-27T08:16:06.218669","exception":false,"start_time":"2024-03-27T08:16:06.203165","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"1922d21a","metadata":{"papermill":{"duration":0.014771,"end_time":"2024-03-27T08:16:06.248444","exception":false,"start_time":"2024-03-27T08:16:06.233673","status":"completed"},"tags":[]},"source":["# Objective\n","\n","\n","## 1. Understanding LLMs\n","**Theoretical Knowledge:** Gain a deep understanding of the architecture, training processes, and underlying algorithms of LLMs, such as transformers, attention mechanisms, and pre-training objectives.\n","\n","**Model Analysis:** Study the behavior of LLMs in various tasks to understand their capabilities, limitations, and the kind of biases they may encode.\n","\n","## 2. Improving LLM Performance\n","**Enhanced Training Techniques:** Explore methods to improve the efficiency and effectiveness of training LLMs, such as advanced optimization techniques, better regularization, or innovative pre-training tasks.\n","\n","**Custom Models:** Develop LLMs tailored to specific languages, domains, or applications where general models may not perform optimally."]},{"cell_type":"markdown","id":"62ecda8e","metadata":{"_cell_guid":"750482ac-b30a-40be-b2a0-54978addee61","_uuid":"a38c4952-764d-4b9c-8e18-52f1184b6861","papermill":{"duration":0.01582,"end_time":"2024-03-27T08:16:06.279046","exception":false,"start_time":"2024-03-27T08:16:06.263226","status":"completed"},"tags":[]},"source":["# Introduction\n","Tokenization is the process of converting a sequence of text into individual units, commonly known as ‘token’. In NLP context, tokens can represent word, subword, or even characters. \n","The primary goal is to prepare raw text data into a format that computational models can more easily analyze.\n","\n","## Role in Large Language Models(LLMs):\n","1. **Traning Phase:**  Data Preprocessing, Sequince Alignments\n","2. **Inference Phase:** Query Understanding, Output Generation"]},{"cell_type":"markdown","id":"23f4a0d1","metadata":{"_cell_guid":"33dbcd17-d887-4c14-bc6f-7c13c755a1b2","_uuid":"dfbabc75-bbf7-4f03-b1c3-ddb38660e108","papermill":{"duration":0.014722,"end_time":"2024-03-27T08:16:06.311146","exception":false,"start_time":"2024-03-27T08:16:06.296424","status":"completed"},"tags":[]},"source":["## Type of Tokenization\n","\n","1. Word Tokenization\n","2. Subword Tokenization\n","3. Character Tokenization\n","4. Morphological Tokenizaton"]},{"cell_type":"markdown","id":"fa5a2eb5","metadata":{"_cell_guid":"7bf9abf5-c0fa-4dac-a98f-49abe1ee80ed","_uuid":"f238c28c-799f-417f-86dd-f0ac1cca4b4f","papermill":{"duration":0.014872,"end_time":"2024-03-27T08:16:06.342383","exception":false,"start_time":"2024-03-27T08:16:06.327511","status":"completed"},"tags":[]},"source":["### 1. Word Tokenization:\n","\n","word Tokenization is one of the earliest and simplest forms of text segmentation. It generally involves splitting a sequence of text into individual word.\n","\n","2 type common algorithm:\n","1. Whitespace Tokenization\n","2. Rule-Based Tokenization"]},{"cell_type":"markdown","id":"a0872137","metadata":{"_cell_guid":"abd5f38a-d555-41b0-9ddf-269519d536ef","_uuid":"02318060-088c-4c4e-9e21-ee88d6c3c675","papermill":{"duration":0.015131,"end_time":"2024-03-27T08:16:06.372564","exception":false,"start_time":"2024-03-27T08:16:06.357433","status":"completed"},"tags":[]},"source":["#### 1.1. Whitespace Tokenization\n","The most basic from of word tokenization is whitespace tokenization, whitch splits text based on spaces."]},{"cell_type":"code","execution_count":1,"id":"1c97ddf1","metadata":{"_cell_guid":"a660a78a-4500-4225-91e0-5bade051428c","_uuid":"a7090380-4e63-45bb-b47f-49329c078aac","collapsed":false,"execution":{"iopub.execute_input":"2024-03-27T08:16:06.404963Z","iopub.status.busy":"2024-03-27T08:16:06.404183Z","iopub.status.idle":"2024-03-27T08:16:06.416433Z","shell.execute_reply":"2024-03-27T08:16:06.415455Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.030922,"end_time":"2024-03-27T08:16:06.418479","exception":false,"start_time":"2024-03-27T08:16:06.387557","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'whitespace', 'tokenization']\n"]}],"source":["# code of whitespace tokenization\n","\n","def whitespace_tokenization(text):\n","    return text.split()\n","\n","text = 'Hello, this is an example text to demonstrate whitespace tokenization'\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"b1a4fd9d","metadata":{"_cell_guid":"e8bfcdfe-c0fc-4c1c-8a32-c024a70e66b5","_uuid":"036ce52a-b68b-4b91-b975-eb5590c57dff","papermill":{"duration":0.015459,"end_time":"2024-03-27T08:16:06.449109","exception":false,"start_time":"2024-03-27T08:16:06.43365","status":"completed"},"tags":[]},"source":["#### 1.2. Rule-Base Tokenization\n","This approach used a set of predefined rules and regex patterns to identify tokens. Example:"]},{"cell_type":"code","execution_count":2,"id":"a0e4888d","metadata":{"_cell_guid":"913c6bc5-6edb-47d7-b9ca-12c38e24e9c1","_uuid":"2567987c-2dc1-4519-8f35-f020aaccef2a","collapsed":false,"execution":{"iopub.execute_input":"2024-03-27T08:16:06.481273Z","iopub.status.busy":"2024-03-27T08:16:06.480588Z","iopub.status.idle":"2024-03-27T08:16:06.48651Z","shell.execute_reply":"2024-03-27T08:16:06.485698Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.024292,"end_time":"2024-03-27T08:16:06.488908","exception":false,"start_time":"2024-03-27T08:16:06.464616","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# code of rule-base tokenization:\n","\n","import re\n","\n","\n","def rule_base_tokenization(text):\n","    # define regex pattern\n","    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n","    \n","    tokens = re.findall(pattern, text)\n","    \n","    return tokens\n","\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"c6e470c5","metadata":{"_cell_guid":"aee8ea5d-9b2b-462e-936c-fa294a22fc7e","_uuid":"105359d5-f858-4c77-8153-f09fb47c1d11","papermill":{"duration":0.015063,"end_time":"2024-03-27T08:16:06.519012","exception":false,"start_time":"2024-03-27T08:16:06.503949","status":"completed"},"tags":[]},"source":["##### comparison between whitespace and rule-based tokenization"]},{"cell_type":"code","execution_count":3,"id":"3c13bf82","metadata":{"_cell_guid":"c1b62537-e848-449a-a1a9-89f20ff79503","_uuid":"ac750ca1-c884-44db-be87-8a0c613e8fe6","collapsed":false,"execution":{"iopub.execute_input":"2024-03-27T08:16:06.550827Z","iopub.status.busy":"2024-03-27T08:16:06.550279Z","iopub.status.idle":"2024-03-27T08:16:06.555583Z","shell.execute_reply":"2024-03-27T08:16:06.554651Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023549,"end_time":"2024-03-27T08:16:06.557653","exception":false,"start_time":"2024-03-27T08:16:06.534104","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens by Whitespace Tokenization:\n"," ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule-based', 'tokenization!', \"Isn't\", 'it', 'great?']\n","Tokens by Rule-base Tokenization:\n"," ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# used common text\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens by Whitespace Tokenization:\\n', tokens)\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens by Rule-base Tokenization:\\n', tokens)"]},{"cell_type":"markdown","id":"0e27f485","metadata":{"_cell_guid":"74110206-112e-4349-8901-f9660fdcf05f","_uuid":"e6d0409d-ddc1-4139-bfeb-dd3490fb38b3","papermill":{"duration":0.015722,"end_time":"2024-03-27T08:16:06.589575","exception":false,"start_time":"2024-03-27T08:16:06.573853","status":"completed"},"tags":[]},"source":["### 2. Subword Tokenization\n","\n","Subword Tokenization techniques operate at a lavel between words and characterss, aiming to capture meaningful linguistic units smaller then a word but large then a character\n","\n","3 common algorithms:\n","\n","1. Byte Pair Encoding(BPE)\n","2. WordPiece \n","3. SentencePiece"]},{"cell_type":"markdown","id":"5918e6f2","metadata":{"_cell_guid":"5ea3227f-8222-4378-9ab9-3fe05da4ea70","_uuid":"89acf4dc-319f-49d8-a4f2-fdd9d510784d","papermill":{"duration":0.015249,"end_time":"2024-03-27T08:16:06.620298","exception":false,"start_time":"2024-03-27T08:16:06.605049","status":"completed"},"tags":[]},"source":["#### 2.1. Byte Pair Encoding(BPE)\n","\n","BPE works by iteratively merginf the most frequently occurring character or character sequences. Following a somplified illustration of how BPE works tokenizing text:\n","* **Initialization:** Start with individual characters or symbols as the basic tokens\n","* **Frequency Count:** Count all adjancent pairs of tokens in the dataset\n","* **Merge:** Identifiy the most frequent pair of tokens and merge them into a single new token\n","* **Repeat:** Repeat the frequency count and merge steps untill a sepecified number of merges has been reached or the vocabulary reaches a desired size\n","\n","Example:\n","\n","Suppose the data to be encoded is\n","> aaabdaaabac\n","\n","The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:\n","> ZabdZabac\n","\n","> Z=aa\n","\n","Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n","> ZYdZYac\n","\n","> Y=ab\n","\n","> Z=aa\n","\n","The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n","> XdXac\n","\n","> X=ZY\n","\n","> Y=ab\n","\n","> Z=aa\n","\n","*This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.*\n","\n","**N.B:** To decompress the data, simply perform the replacements in the reverse order.\n","Below is a basic python implementation of BPE:"]},{"cell_type":"code","execution_count":4,"id":"23b93655","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:06.651871Z","iopub.status.busy":"2024-03-27T08:16:06.651604Z","iopub.status.idle":"2024-03-27T08:16:06.65974Z","shell.execute_reply":"2024-03-27T08:16:06.658921Z"},"papermill":{"duration":0.026355,"end_time":"2024-03-27T08:16:06.661812","exception":false,"start_time":"2024-03-27T08:16:06.635457","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'this': 1, 'is': 1, 'an': 2, 'example.': 1, 'I': 1, 'am': 1, 'engineer': 1}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from collections import defaultdict, Counter\n","\n","def get_vocab(text):\n","    \"\"\"Split text into symbles and connt symbols frequencies\"\"\"\n","    \n","    vocab = Counter(text.split())\n","    \n","    #convert vocabulary to format {'word': count}\n","    \n","    return {word: freq for word, freq in vocab.items()}\n","\n","get_vocab('this is an example. I am an engineer')\n","    "]},{"cell_type":"code","execution_count":5,"id":"a88678d9","metadata":{"_cell_guid":"894656c7-c18c-40be-b1cf-4abf83b43c4f","_uuid":"7b3b553c-5c7a-429e-81b8-30a7fda47819","collapsed":false,"execution":{"iopub.execute_input":"2024-03-27T08:16:06.694649Z","iopub.status.busy":"2024-03-27T08:16:06.694067Z","iopub.status.idle":"2024-03-27T08:16:06.701675Z","shell.execute_reply":"2024-03-27T08:16:06.700801Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.026165,"end_time":"2024-03-27T08:16:06.703877","exception":false,"start_time":"2024-03-27T08:16:06.677712","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["defaultdict(int, {})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","def get_stats(vocab):\n","    \n","    pairs = defaultdict(int)\n","\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i  in range(len(symbols) - 1):\n","            pairs[symbols[i], symbols[i+1]] += freq\n","    return pairs\n","\n","vocab = get_vocab('this is an example. I am an engineer.')\n","get_stats(vocab)"]},{"cell_type":"code","execution_count":6,"id":"dc498637","metadata":{"_cell_guid":"79b8a12c-42e0-4db7-a701-a1a8eb1d3d65","_uuid":"430c4da5-4e20-47f6-aa81-ebcf754b7dc7","collapsed":false,"execution":{"iopub.execute_input":"2024-03-27T08:16:06.736698Z","iopub.status.busy":"2024-03-27T08:16:06.736404Z","iopub.status.idle":"2024-03-27T08:16:06.741186Z","shell.execute_reply":"2024-03-27T08:16:06.740403Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.023612,"end_time":"2024-03-27T08:16:06.743068","exception":false,"start_time":"2024-03-27T08:16:06.719456","status":"completed"},"tags":[]},"outputs":[],"source":["def marge_vocab(pair, vocab):\n","    new_vocab = {}\n","    bigram = ' '.join(pair)\n","    replacement = ''.join(pair)\n","    for word in vocab:\n","        new_word = word.replace(bigram, replacement)\n","        new_vocab[new_word] = vocab[word]\n","        \n","    return new_vocab\n","\n"]},{"cell_type":"code","execution_count":7,"id":"4ccf1ce9","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:06.775346Z","iopub.status.busy":"2024-03-27T08:16:06.775043Z","iopub.status.idle":"2024-03-27T08:16:06.783048Z","shell.execute_reply":"2024-03-27T08:16:06.782216Z"},"papermill":{"duration":0.026308,"end_time":"2024-03-27T08:16:06.784915","exception":false,"start_time":"2024-03-27T08:16:06.758607","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'I', 'am', 'an', 'engineer.', 'example.', 'is', 'test', 'this'}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["def bpe_tokenize(text, number_merges):\n","    vocab = get_vocab(text)\n","    for i in range(number_merges):\n","        pairs = get_stats(vocab)\n","        if not pairs:\n","            break\n","        best = max(pairs, key=pairs.get)\n","        vocab = marge_vocab(best, vocab)\n","        \n","    tokens = set()\n","    for word in vocab:\n","        tokens.update(word.split())\n","        \n","    return tokens\n","\n","text = 'this is an example. I am an engineer. this is test'\n","\n","tokens = bpe_tokenize(text, 10)\n","tokens"]},{"cell_type":"markdown","id":"5ec42dde","metadata":{"papermill":{"duration":0.015638,"end_time":"2024-03-27T08:16:06.816116","exception":false,"start_time":"2024-03-27T08:16:06.800478","status":"completed"},"tags":[]},"source":["##### For LLMs"]},{"cell_type":"code","execution_count":8,"id":"81825c22","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:06.850277Z","iopub.status.busy":"2024-03-27T08:16:06.849957Z","iopub.status.idle":"2024-03-27T08:16:06.855873Z","shell.execute_reply":"2024-03-27T08:16:06.854991Z"},"papermill":{"duration":0.025227,"end_time":"2024-03-27T08:16:06.857782","exception":false,"start_time":"2024-03-27T08:16:06.832555","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[116, 104, 105, 115, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 73, 32, 97, 109, 32, 97, 110, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46, 32, 116, 104, 105, 115, 32, 105, 115, 32, 116, 101, 115, 116]\n","\n","len of text 50\n","len of token 50\n"]}],"source":["def encoding(text):\n","    tokens = text.encode('utf-8') # Raw bytes\n","    return list(map(int, tokens))\n","\n","text = 'this is an example. I am an engineer. this is test'\n","tokens = encoding(text)\n","print(tokens)\n","print('\\nlen of text', len(text))\n","print('len of token', len(tokens))\n","\n"]},{"cell_type":"code","execution_count":9,"id":"7e9bdb4f","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:06.891575Z","iopub.status.busy":"2024-03-27T08:16:06.890989Z","iopub.status.idle":"2024-03-27T08:16:06.896486Z","shell.execute_reply":"2024-03-27T08:16:06.895532Z"},"papermill":{"duration":0.024985,"end_time":"2024-03-27T08:16:06.898857","exception":false,"start_time":"2024-03-27T08:16:06.873872","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{(116, 104): 2, (104, 105): 2, (105, 115): 4, (115, 32): 4, (32, 105): 2, (32, 97): 3, (97, 110): 2, (110, 32): 2, (32, 101): 2, (101, 120): 1, (120, 97): 1, (97, 109): 2, (109, 112): 1, (112, 108): 1, (108, 101): 1, (101, 46): 1, (46, 32): 2, (32, 73): 1, (73, 32): 1, (109, 32): 1, (101, 110): 1, (110, 103): 1, (103, 105): 1, (105, 110): 1, (110, 101): 1, (101, 101): 1, (101, 114): 1, (114, 46): 1, (32, 116): 2, (116, 101): 1, (101, 115): 1, (115, 116): 1}\n"]}],"source":["def get_stats(ids):\n","    counts = {}\n","    for pair in zip(ids, ids[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","    return counts\n","\n","stats = get_stats(tokens)\n","\n","print(stats)"]},{"cell_type":"code","execution_count":10,"id":"8c17bad9","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:06.931629Z","iopub.status.busy":"2024-03-27T08:16:06.931056Z","iopub.status.idle":"2024-03-27T08:16:06.936432Z","shell.execute_reply":"2024-03-27T08:16:06.935672Z"},"papermill":{"duration":0.023795,"end_time":"2024-03-27T08:16:06.938417","exception":false,"start_time":"2024-03-27T08:16:06.914622","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(105, 115)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["top_pair = max(stats, key=stats.get)\n","top_pair"]},{"cell_type":"code","execution_count":11,"id":"159e376f","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:07.011832Z","iopub.status.busy":"2024-03-27T08:16:07.011035Z","iopub.status.idle":"2024-03-27T08:16:07.01671Z","shell.execute_reply":"2024-03-27T08:16:07.015948Z"},"papermill":{"duration":0.024473,"end_time":"2024-03-27T08:16:07.018537","exception":false,"start_time":"2024-03-27T08:16:06.994064","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["('i', 's')"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["chr(105), chr(115)"]},{"cell_type":"code","execution_count":12,"id":"39b4a4a5","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:07.051951Z","iopub.status.busy":"2024-03-27T08:16:07.051696Z","iopub.status.idle":"2024-03-27T08:16:07.059703Z","shell.execute_reply":"2024-03-27T08:16:07.058915Z"},"papermill":{"duration":0.02697,"end_time":"2024-03-27T08:16:07.061567","exception":false,"start_time":"2024-03-27T08:16:07.034597","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[5, 6, 99, 9, 1]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["def merge(ids, pair, idx):\n","    new_ids = []\n","    \n","    i = 0\n","    while i < len(ids):\n","        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n","            new_ids.append(idx)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids\n","\n","merge([5, 6, 6, 7, 9, 1], (6, 7), 99)"]},{"cell_type":"code","execution_count":13,"id":"2f0579b4","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:07.094886Z","iopub.status.busy":"2024-03-27T08:16:07.094653Z","iopub.status.idle":"2024-03-27T08:16:07.100061Z","shell.execute_reply":"2024-03-27T08:16:07.099181Z"},"papermill":{"duration":0.024073,"end_time":"2024-03-27T08:16:07.101845","exception":false,"start_time":"2024-03-27T08:16:07.077772","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[116, 104, 256, 32, 256, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 73, 32, 97, 109, 32, 97, 110, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46, 32, 116, 104, 256, 32, 256, 32, 116, 101, 115, 116]\n","\n","len of token2: 46\n"]}],"source":["tokens2 = merge(tokens, top_pair, 256)\n","print(tokens2)\n","\n","print('\\nlen of token2:', len(tokens2))\n"]},{"cell_type":"markdown","id":"d6917e69","metadata":{"papermill":{"duration":0.016845,"end_time":"2024-03-27T08:16:07.136268","exception":false,"start_time":"2024-03-27T08:16:07.119423","status":"completed"},"tags":[]},"source":["##### Make merge dataset"]},{"cell_type":"code","execution_count":14,"id":"40bc36dd","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:16:07.170879Z","iopub.status.busy":"2024-03-27T08:16:07.170593Z","iopub.status.idle":"2024-03-27T08:19:31.527204Z","shell.execute_reply":"2024-03-27T08:19:31.526462Z"},"papermill":{"duration":204.376679,"end_time":"2024-03-27T08:19:31.529583","exception":false,"start_time":"2024-03-27T08:16:07.152904","status":"completed"},"tags":[]},"outputs":[],"source":["#using constant time\n","vocab_size = 5000\n","number_megres = vocab_size - 256\n","\n","with open('/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt', 'r') as file:\n","    text = file.read()\n","    \n","tokens = encoding(text)\n","ids = tokens\n","\n","merges = {}\n","\n","for i in range(number_megres):\n","    stats = get_stats(ids)\n","    \n","    pair = max(stats, key=stats.get)\n","    if stats[pair] == 1:\n","        break\n","    idx = 256 + i\n","    #print(f\"Merging: {pair} in a new token {idx}\")\n","    ids = merge(ids, pair, idx)\n","    merges[pair] = idx"]},{"cell_type":"markdown","id":"0d0ff89e","metadata":{"papermill":{"duration":0.016021,"end_time":"2024-03-27T08:19:31.562603","exception":false,"start_time":"2024-03-27T08:19:31.546582","status":"completed"},"tags":[]},"source":["##### Evulation"]},{"cell_type":"code","execution_count":15,"id":"229dc442","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:31.5965Z","iopub.status.busy":"2024-03-27T08:19:31.596069Z","iopub.status.idle":"2024-03-27T08:19:31.601401Z","shell.execute_reply":"2024-03-27T08:19:31.600517Z"},"papermill":{"duration":0.025118,"end_time":"2024-03-27T08:19:31.603821","exception":false,"start_time":"2024-03-27T08:19:31.578703","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["token len 141695\n","ids len 31546\n","compression ratio:  4.4916946681037215\n"]}],"source":["print('token len', len(tokens))\n","print('ids len', len(ids))\n","print('compression ratio: ', len(tokens)/len(ids))"]},{"cell_type":"markdown","id":"d510049d","metadata":{"papermill":{"duration":0.016241,"end_time":"2024-03-27T08:19:31.636671","exception":false,"start_time":"2024-03-27T08:19:31.62043","status":"completed"},"tags":[]},"source":["##### Merges download dataset"]},{"cell_type":"code","execution_count":16,"id":"176a329b","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:31.671274Z","iopub.status.busy":"2024-03-27T08:19:31.670942Z","iopub.status.idle":"2024-03-27T08:19:31.678483Z","shell.execute_reply":"2024-03-27T08:19:31.677774Z"},"papermill":{"duration":0.026995,"end_time":"2024-03-27T08:19:31.680393","exception":false,"start_time":"2024-03-27T08:19:31.653398","status":"completed"},"tags":[]},"outputs":[],"source":["with open('merges.txt', 'w') as file:\n","    file.write(str(merges))"]},{"cell_type":"markdown","id":"eb857c5c","metadata":{"papermill":{"duration":0.016165,"end_time":"2024-03-27T08:19:31.712819","exception":false,"start_time":"2024-03-27T08:19:31.696654","status":"completed"},"tags":[]},"source":["##### load merges data"]},{"cell_type":"code","execution_count":17,"id":"11d59985","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:31.747347Z","iopub.status.busy":"2024-03-27T08:19:31.74698Z","iopub.status.idle":"2024-03-27T08:19:31.84682Z","shell.execute_reply":"2024-03-27T08:19:31.846007Z"},"papermill":{"duration":0.119066,"end_time":"2024-03-27T08:19:31.848998","exception":false,"start_time":"2024-03-27T08:19:31.729932","status":"completed"},"tags":[]},"outputs":[],"source":["import ast\n","\n","with open('merges.txt', 'r') as file:\n","    data_string = file.read()\n","    merges = ast.literal_eval(data_string)\n"]},{"cell_type":"markdown","id":"3a6830d3","metadata":{"papermill":{"duration":0.016185,"end_time":"2024-03-27T08:19:31.882023","exception":false,"start_time":"2024-03-27T08:19:31.865838","status":"completed"},"tags":[]},"source":["##### Encoding"]},{"cell_type":"code","execution_count":18,"id":"b1330b8b","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:31.916582Z","iopub.status.busy":"2024-03-27T08:19:31.916013Z","iopub.status.idle":"2024-03-27T08:19:31.923852Z","shell.execute_reply":"2024-03-27T08:19:31.922993Z"},"papermill":{"duration":0.0272,"end_time":"2024-03-27T08:19:31.925703","exception":false,"start_time":"2024-03-27T08:19:31.898503","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[97]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["def encode(text):\n","    tokens = list(text.encode('utf-8'))\n","    \n","    while len(tokens)>=2:\n","        stats = get_stats(tokens)\n","        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n","        \n","        if pair not in merges:\n","            break\n","        \n","        idx = merges[pair]\n","        tokens = merge(tokens, pair, idx)\n","        \n","        \n","    return tokens\n","\n","encode('a')"]},{"cell_type":"markdown","id":"e36846bb","metadata":{"papermill":{"duration":0.01617,"end_time":"2024-03-27T08:19:31.958244","exception":false,"start_time":"2024-03-27T08:19:31.942074","status":"completed"},"tags":[]},"source":["##### Vocab download"]},{"cell_type":"code","execution_count":19,"id":"664471c5","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:31.992415Z","iopub.status.busy":"2024-03-27T08:19:31.9919Z","iopub.status.idle":"2024-03-27T08:19:32.00129Z","shell.execute_reply":"2024-03-27T08:19:32.000631Z"},"papermill":{"duration":0.028604,"end_time":"2024-03-27T08:19:32.003102","exception":false,"start_time":"2024-03-27T08:19:31.974498","status":"completed"},"tags":[]},"outputs":[],"source":["vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","for (p0, p1), idx in merges.items():\n","    vocab[idx] = vocab[p0] + vocab[p1]\n","    \n","with open('vocab.txt', 'w') as file:\n","    file.write(str(vocab))\n"]},{"cell_type":"markdown","id":"d75d56ec","metadata":{"papermill":{"duration":0.016247,"end_time":"2024-03-27T08:19:32.03579","exception":false,"start_time":"2024-03-27T08:19:32.019543","status":"completed"},"tags":[]},"source":["##### Load Vocab"]},{"cell_type":"code","execution_count":20,"id":"91ce8d5d","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.071416Z","iopub.status.busy":"2024-03-27T08:19:32.071071Z","iopub.status.idle":"2024-03-27T08:19:32.099078Z","shell.execute_reply":"2024-03-27T08:19:32.098244Z"},"papermill":{"duration":0.047602,"end_time":"2024-03-27T08:19:32.101073","exception":false,"start_time":"2024-03-27T08:19:32.053471","status":"completed"},"tags":[]},"outputs":[],"source":["import ast\n","\n","with open('vocab.txt', 'r') as file:\n","    data_string = file.read()\n","    vocab = ast.literal_eval(data_string)\n","\n","\n"]},{"cell_type":"markdown","id":"8f107b59","metadata":{"papermill":{"duration":0.016537,"end_time":"2024-03-27T08:19:32.134272","exception":false,"start_time":"2024-03-27T08:19:32.117735","status":"completed"},"tags":[]},"source":["##### Decoding"]},{"cell_type":"code","execution_count":21,"id":"e82212fd","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.169875Z","iopub.status.busy":"2024-03-27T08:19:32.169077Z","iopub.status.idle":"2024-03-27T08:19:32.174796Z","shell.execute_reply":"2024-03-27T08:19:32.173885Z"},"papermill":{"duration":0.025372,"end_time":"2024-03-27T08:19:32.176788","exception":false,"start_time":"2024-03-27T08:19:32.151416","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["a\n"]}],"source":["\n","def decode(ids):\n","    tokens = b''.join(vocab[idx] for idx in ids)\n","    text = tokens.decode('utf-8', errors='replace')\n","    \n","    return text\n","\n","\n","print(decode([97]))"]},{"cell_type":"code","execution_count":22,"id":"ba9accf2","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.213165Z","iopub.status.busy":"2024-03-27T08:19:32.21255Z","iopub.status.idle":"2024-03-27T08:19:32.218166Z","shell.execute_reply":"2024-03-27T08:19:32.217386Z"},"papermill":{"duration":0.025956,"end_time":"2024-03-27T08:19:32.220103","exception":false,"start_time":"2024-03-27T08:19:32.194147","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'Hello World!'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["decode(encode('Hello World!'))"]},{"cell_type":"markdown","id":"48d304ed","metadata":{"papermill":{"duration":0.016686,"end_time":"2024-03-27T08:19:32.253718","exception":false,"start_time":"2024-03-27T08:19:32.237032","status":"completed"},"tags":[]},"source":["#### 2.2. WordPiece\n","\n","Word Piece Tokenization is a method used primarily in natural language processing (NLP) tasks to split text into manageable pieces or tokens. This technique is especially beneficial for dealing with languages that have a rich morphology or for processing text that includes a lot of unfamiliar words or names. The core idea behind Word Piece Tokenization is to balance between the granularity of character-level tokenization and the broader scope of word-level tokenization, effectively improving model performance in various NLP tasks.\n","\n","\n","Tokenization Process:\n","\n","1. Starting Point: Words are first checked against the vocabulary. If a word is found, it is used as a token as is.\n","\n","2. Subword Segmentation: For words not found in the vocabulary, the algorithm attempts to break the word into the largest possible subwords available in the vocabulary. This step is iterated until the whole word is segmented into known subwords or characters.\n","\n","3. Rare Words Handling: The inclusion of characters in the vocabulary ensures that any word, no matter how uncommon or complex, can be tokenized into familiar units, allowing the model to attempt understanding and processing it.\n","\n","\n","Let's consider the word \"unhappiness\" to illustrate the process of WordPiece Tokenization with a more complex example. We'll assume our model's vocabulary includes the following tokens, among others:\n","\n","> un\n","\n","> ##happy\n","\n","> ##ness\n","\n","> happiness\n","\n","Given the word \"unhappiness\", here's how WordPiece Tokenization would proceed:\n","\n","Full Word Matching: The tokenizer first checks if \"unhappiness\" is present as a complete word in its vocabulary. If \"unhappiness\" isn't found as a whole word, the tokenizer moves to break down the word into known subwords or tokens.\n","\n","Subword Segmentation:\n","\n","The tokenizer identifies the prefix \"un\" as a common prefix in its vocabulary, indicating negation or the opposite of the base word.\n","Next, it looks for the longest possible matching sequence that follows \"un\". While \"happiness\" might be in the vocabulary, the tokenizer efficiently breaks down complex words into smaller subwords to maximize vocabulary usage and handle variations robustly.\n","After \"un\", it might identify \"happy\" as a base word. However, since \"happy\" is not standalone in this context, the tokenizer uses the token ##happy to indicate that \"happy\" is part of a larger word.\n","Finally, it recognizes the suffix \"ness\", which is represented in the vocabulary as ##ness to indicate that it's attached to another token.\n","Tokenization Output: Combining these tokens, the process results in the following sequence:\n","\n","> un, \n","\n","> ##happy, \n","\n","> ##ness\n"]},{"cell_type":"code","execution_count":23,"id":"2094cb22","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.288935Z","iopub.status.busy":"2024-03-27T08:19:32.28817Z","iopub.status.idle":"2024-03-27T08:19:32.29954Z","shell.execute_reply":"2024-03-27T08:19:32.298878Z"},"papermill":{"duration":0.031183,"end_time":"2024-03-27T08:19:32.301492","exception":false,"start_time":"2024-03-27T08:19:32.270309","status":"completed"},"tags":[]},"outputs":[],"source":["with open('/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt', 'r') as file:\n","    text = file.read()"]},{"cell_type":"code","execution_count":24,"id":"bfa7b331","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.336573Z","iopub.status.busy":"2024-03-27T08:19:32.336301Z","iopub.status.idle":"2024-03-27T08:19:32.520234Z","shell.execute_reply":"2024-03-27T08:19:32.519481Z"},"papermill":{"duration":0.204091,"end_time":"2024-03-27T08:19:32.522496","exception":false,"start_time":"2024-03-27T08:19:32.318405","status":"completed"},"tags":[]},"outputs":[],"source":["corpus = text.split()\n","\n","# Tokenize the corpus into words (naive approach for simplicity)\n","words = [word.lower() for sentence in corpus for word in sentence.split()]\n","\n","# Further break down into subwords/characters (naive splitting for demonstration)\n","subwords = set()\n","for word in words:\n","    length = len(word)\n","    for i in range(length):\n","        for j in range(i+1, length+1):\n","            subwords.add(word[i:j])\n","            "]},{"cell_type":"code","execution_count":25,"id":"d8646d14","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.557916Z","iopub.status.busy":"2024-03-27T08:19:32.55737Z","iopub.status.idle":"2024-03-27T08:19:32.598884Z","shell.execute_reply":"2024-03-27T08:19:32.598178Z"},"papermill":{"duration":0.061236,"end_time":"2024-03-27T08:19:32.600805","exception":false,"start_time":"2024-03-27T08:19:32.539569","status":"completed"},"tags":[]},"outputs":[],"source":["vocab = {idx: chr(idx) for idx in range(256)}\n","\n","for i, words in enumerate(subwords):\n","    if words not in vocab.items():\n","        vocab[256 + i] = words\n","        \n","with open('word piece vocab.txt', 'w') as file:\n","    file.write(str(vocab))"]},{"cell_type":"code","execution_count":26,"id":"6ee9a4c8","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:32.63555Z","iopub.status.busy":"2024-03-27T08:19:32.635223Z","iopub.status.idle":"2024-03-27T08:19:32.972262Z","shell.execute_reply":"2024-03-27T08:19:32.971372Z"},"papermill":{"duration":0.356949,"end_time":"2024-03-27T08:19:32.974642","exception":false,"start_time":"2024-03-27T08:19:32.617693","status":"completed"},"tags":[]},"outputs":[],"source":["# Load Vocab\n","with open('word piece vocab.txt', 'r', encoding='utf-8') as file:\n","    data_string = file.read()\n","    vocab = ast.literal_eval(data_string)"]},{"cell_type":"code","execution_count":27,"id":"467fa06a","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.010095Z","iopub.status.busy":"2024-03-27T08:19:33.009752Z","iopub.status.idle":"2024-03-27T08:19:33.01639Z","shell.execute_reply":"2024-03-27T08:19:33.015564Z"},"papermill":{"duration":0.026388,"end_time":"2024-03-27T08:19:33.018309","exception":false,"start_time":"2024-03-27T08:19:32.991921","status":"completed"},"tags":[]},"outputs":[],"source":["\n","def encode_text(text, vocab):\n","    # Reverse the vocab to map items to their indexes for encoding\n","    item_to_index = {v: k for k, v in vocab.items()}\n","    \n","    encoded_ids = []\n","    for word in text.split():\n","        encoded_word = False\n","        for i in range(len(word), 0, -1):\n","            if word[:i] in item_to_index.keys():\n","                encoded_ids.append(item_to_index[word[:i]])\n","                word = word[i:]\n","        if not encoded_word:  # If part of the word wasn't in the vocab\n","            for char in word:  # Fallback to character encoding\n","                if char in item_to_index:\n","                    encoded_ids.append(item_to_index[char])\n","    return encoded_ids\n"]},{"cell_type":"code","execution_count":28,"id":"c9834676","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.053765Z","iopub.status.busy":"2024-03-27T08:19:33.052992Z","iopub.status.idle":"2024-03-27T08:19:33.058013Z","shell.execute_reply":"2024-03-27T08:19:33.057144Z"},"papermill":{"duration":0.024661,"end_time":"2024-03-27T08:19:33.059883","exception":false,"start_time":"2024-03-27T08:19:33.035222","status":"completed"},"tags":[]},"outputs":[],"source":["# Decoder\n","def decode_ids(encoded_ids, vocab):\n","    decoded_text = ''\n","    for id in encoded_ids:\n","        decoded_text += vocab[id]\n","    return decoded_text\n"]},{"cell_type":"code","execution_count":29,"id":"53c8d806","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.094271Z","iopub.status.busy":"2024-03-27T08:19:33.093974Z","iopub.status.idle":"2024-03-27T08:19:33.106611Z","shell.execute_reply":"2024-03-27T08:19:33.10576Z"},"papermill":{"duration":0.031946,"end_time":"2024-03-27T08:19:33.108518","exception":false,"start_time":"2024-03-27T08:19:33.076572","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded Tokens: [87, 26014, 17726, 20052, 80, 15083, 9228, 33776, 9228, 10383, 40118, 38237, 26909, 9228, 33843, 15083, 24012, 23636, 17444, 15083, 26014, 33843]\n","decoded_text: WordPiecehelpsintokenization\n"]}],"source":["text2 = \"WordPiece helps in tokenization\"\n","\n","encoded_tokens = encode_text(text2, vocab)\n","decoded_text = decode_ids(encoded_tokens, vocab)\n","\n","print(\"Encoded Tokens:\", encoded_tokens)\n","print(\"decoded_text:\", decoded_text)"]},{"cell_type":"code","execution_count":30,"id":"834f6d8c","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.143884Z","iopub.status.busy":"2024-03-27T08:19:33.143595Z","iopub.status.idle":"2024-03-27T08:19:33.148237Z","shell.execute_reply":"2024-03-27T08:19:33.14742Z"},"papermill":{"duration":0.024896,"end_time":"2024-03-27T08:19:33.150328","exception":false,"start_time":"2024-03-27T08:19:33.125432","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["token len 141695\n","ids len 43033\n","compression ratio:  3.292705598029419\n"]}],"source":["print('token len', len(text))\n","print('ids len', len(vocab))\n","print('compression ratio: ', len(text)/len(vocab))"]},{"cell_type":"markdown","id":"a71cd758","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:20:10.497972Z","iopub.status.busy":"2024-03-25T04:20:10.497334Z","iopub.status.idle":"2024-03-25T04:20:10.501798Z","shell.execute_reply":"2024-03-25T04:20:10.500872Z","shell.execute_reply.started":"2024-03-25T04:20:10.497939Z"},"papermill":{"duration":0.016474,"end_time":"2024-03-27T08:19:33.183725","exception":false,"start_time":"2024-03-27T08:19:33.167251","status":"completed"},"tags":[]},"source":["#  Unigram Model Language Tokenization"]},{"cell_type":"code","execution_count":31,"id":"13ab621c","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.219838Z","iopub.status.busy":"2024-03-27T08:19:33.219522Z","iopub.status.idle":"2024-03-27T08:19:33.233118Z","shell.execute_reply":"2024-03-27T08:19:33.232287Z"},"papermill":{"duration":0.034674,"end_time":"2024-03-27T08:19:33.235304","exception":false,"start_time":"2024-03-27T08:19:33.20063","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded text: [11, 1, 12, 2, 13, 3]\n"]}],"source":["import re\n","from collections import Counter, defaultdict\n","\n","# Example corpus\n","corpus = \"\"\"\n","Romeo and Juliet\n","by William Shakespeare\n","Edited by Barbara A. Mowat and Paul Werstine\n","with Michael Poston and Rebecca Niles\n","Folger Shakespeare Library\n","\"\"\"\n","\n","# Preprocessing: Tokenize the corpus into words\n","words = re.findall(r'\\w+', corpus.lower())\n","\n","# Generate subwords for each word in a naive way\n","def generate_subwords(word):\n","    subwords = set()\n","    for i in range(len(word)):\n","        for j in range(i + 1, len(word) + 1):\n","            if len(word[i:j]) > 1:  # Considering subwords of length > 1\n","                subwords.add(word[i:j])\n","    return subwords\n","\n","# Create a frequency dictionary for all words and subwords\n","frequency_dict = Counter(words)\n","for word in words:\n","    for subword in generate_subwords(word):\n","        frequency_dict[subword] += 1\n","\n","# Sort subwords by frequency and pick the top N\n","N = 10000  # Limit on the vocabulary size for demonstration\n","vocabulary = {word: i for i, (word, _) in enumerate(frequency_dict.most_common(N), start=1)}\n","\n","# Tokenization function using the generated vocabulary\n","def tokenize(text, vocab):\n","    tokens = []\n","    while text:\n","        matched = False\n","        # Attempt to match the longest possible token at each step\n","        for length in range(len(text), 0, -1):\n","            if text[:length] in vocab:\n","                tokens.append(vocab[text[:length]])\n","                text = text[length:]\n","                matched = True\n","                break\n","        if not matched:\n","            text = text[1:]  # Skip the character if no match found\n","    return tokens\n","\n","# Testing the tokenization\n","test_text = \"Romeo and Juliet by William Shakespeare\"\n","encoded_text = tokenize(test_text.lower(), vocabulary)\n","print(\"Encoded text:\", encoded_text)\n"]},{"cell_type":"code","execution_count":32,"id":"8d83a192","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.271931Z","iopub.status.busy":"2024-03-27T08:19:33.271676Z","iopub.status.idle":"2024-03-27T08:19:33.278111Z","shell.execute_reply":"2024-03-27T08:19:33.277259Z"},"papermill":{"duration":0.027726,"end_time":"2024-03-27T08:19:33.280323","exception":false,"start_time":"2024-03-27T08:19:33.252597","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Decoded text: romeoandjulietbywilliamshakespeare\n"]}],"source":["# Assuming `vocabulary` is the vocabulary dictionary mapping tokens to unique IDs as before\n","\n","# Invert the vocabulary to map IDs back to tokens\n","id_to_vocab = {id: token for token, id in vocabulary.items()}\n","\n","# Decoding function\n","def decode(token_ids, id_to_vocab):\n","    decoded_tokens = [id_to_vocab[id] for id in token_ids]\n","    \n","    # Reconstruct the text from tokens\n","    decoded_text = ''.join(decoded_tokens)\n","    \n","    # Post-processing to handle subwords correctly.\n","    # For this simplistic approach, let's just replace boundaries with spaces and strip leading/trailing spaces\n","    # Note: This might not perfectly reconstruct the original text due to the simplistic nature of the tokenization\n","    decoded_text = decoded_text.replace('▁', ' ').strip()\n","    \n","    return decoded_text\n","\n","# Example usage with the previously encoded text\n","decoded_text = decode(encoded_text, id_to_vocab)\n","print(\"Decoded text:\", decoded_text)\n"]},{"cell_type":"markdown","id":"efc4bedb","metadata":{"papermill":{"duration":0.016693,"end_time":"2024-03-27T08:19:33.314017","exception":false,"start_time":"2024-03-27T08:19:33.297324","status":"completed"},"tags":[]},"source":["# SentencePiece\n","\n","The SentencePiece library offers a robust and flexible solution for tokenizing text into subword units such as characters, subwords, or words without requiring pre-tokenization. Developed by Google, it's designed to be language-agnostic, making it suitable for processing text in any language, which is particularly beneficial for neural network models in natural language processing (NLP) tasks.\n","\n","Key Features of SentencePiece:\n","Unsupervised Tokenization: SentencePiece can train tokenization models directly on raw text data, eliminating the need for language-specific tokenizers.\n","Subword Tokenization: It supports subword tokenization methods, including Byte Pair Encoding (BPE) and Unigram language models, helping to address issues with out-of-vocabulary words by breaking down words into meaningful subunits.\n","Language Agnostic: Its design doesn't rely on whitespace to determine word boundaries, making it effective for languages without clear word delimiters.\n","Versatile Applications: SentencePiece is widely used in various NLP tasks, including machine translation, text classification, and more, due to its effectiveness in managing vocabulary sizes and handling unseen words."]},{"cell_type":"code","execution_count":33,"id":"16ce6efe","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:33.349136Z","iopub.status.busy":"2024-03-27T08:19:33.348821Z","iopub.status.idle":"2024-03-27T08:19:47.13059Z","shell.execute_reply":"2024-03-27T08:19:47.12951Z"},"papermill":{"duration":13.802249,"end_time":"2024-03-27T08:19:47.132987","exception":false,"start_time":"2024-03-27T08:19:33.330738","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\r\n"]}],"source":["!pip install sentencepiece\n","import sentencepiece as spm"]},{"cell_type":"code","execution_count":34,"id":"182cc23f","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:47.16997Z","iopub.status.busy":"2024-03-27T08:19:47.169621Z","iopub.status.idle":"2024-03-27T08:19:47.351773Z","shell.execute_reply":"2024-03-27T08:19:47.350476Z"},"papermill":{"duration":0.20523,"end_time":"2024-03-27T08:19:47.355705","exception":false,"start_time":"2024-03-27T08:19:47.150475","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt\n"]},{"name":"stderr","output_type":"stream","text":["sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt --model_prefix=m --vocab_size=3493 --model_type=unigram\n","sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: /kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt\n","  input_format: \n","  model_prefix: m\n","  model_type: UNIGRAM\n","  vocab_size: 3493\n","  self_test_sample_size: 0\n","  character_coverage: 0.9995\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 0\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  seed_sentencepieces_file: \n","  hard_vocab_limit: 1\n","  use_all_vocab: 0\n","  unk_id: 0\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: -1\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(185) LOG(INFO) Loading corpus: /kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt\n","trainer_interface.cc(409) LOG(INFO) Loaded all 5011 sentences\n","trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(539) LOG(INFO) all chars count=140361\n","trainer_interface.cc(550) LOG(INFO) Done: 99.958% characters are covered.\n","trainer_interface.cc(560) LOG(INFO) Alphabet size=63\n","trainer_interface.cc(561) LOG(INFO) Final character coverage=0.99958\n","trainer_interface.cc(592) LOG(INFO) Done! preprocessed 4155 sentences.\n","unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n","unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=71331\n","unigram_model_trainer.cc(312) LOG(INFO) Initialized 9330 seed sentencepieces\n","trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 4155\n","trainer_interface.cc(609) LOG(INFO) Done! 6288\n","unigram_model_trainer.cc(602) LOG(INFO) Using 6288 sentences for EM training\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=3981 obj=11.2595 num_tokens=13033 num_tokens/piece=3.2738\n","unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=3477 obj=9.43584 num_tokens=13089 num_tokens/piece=3.76445\n","trainer_interface.cc(687) LOG(INFO) Saving model: m.model\n","trainer_interface.cc(699) LOG(INFO) Saving vocabs: m.vocab\n"]}],"source":["corpus_path = '/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt'\n","print(corpus_path)\n","spm.SentencePieceTrainer.train(f'--input={corpus_path} --model_prefix=m --vocab_size=3493 --model_type=unigram')"]},{"cell_type":"code","execution_count":35,"id":"a6cb25d1","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:47.392365Z","iopub.status.busy":"2024-03-27T08:19:47.391608Z","iopub.status.idle":"2024-03-27T08:19:47.472222Z","shell.execute_reply":"2024-03-27T08:19:47.471135Z"},"papermill":{"duration":0.100753,"end_time":"2024-03-27T08:19:47.474098","exception":false,"start_time":"2024-03-27T08:19:47.373345","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['▁Romeo', '▁and', '▁Juliet', '▁by', '▁Will', 'ia', 'm', '▁Shakespeare', '▁E', 'dite']\n","[36, 9, 101, 72, 504, 1830, 80, 1877, 343, 2539]\n"]}],"source":["# Initialize the model\n","sp = spm.SentencePieceProcessor(model_file='m.model')\n","\n","# Tokenize text to subword pieces\n","tokens = sp.encode_as_pieces(text)\n","print(tokens[:10])\n","\n","# Tokenize text to subword IDs\n","ids = sp.encode_as_ids(text)\n","print(ids[:10])\n"]},{"cell_type":"code","execution_count":36,"id":"30353a89","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:47.511073Z","iopub.status.busy":"2024-03-27T08:19:47.510793Z","iopub.status.idle":"2024-03-27T08:19:47.582158Z","shell.execute_reply":"2024-03-27T08:19:47.581187Z"},"papermill":{"duration":0.092265,"end_time":"2024-03-27T08:19:47.584114","exception":false,"start_time":"2024-03-27T08:19:47.491849","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Romeo and \n","Romeo and \n"]}],"source":["# Detokenize subword pieces back to text\n","detokenized_text = sp.decode_pieces(tokens)\n","print(detokenized_text[:10])\n","\n","# Detokenize subword IDs back to text\n","detokenized_text_ids = sp.decode_ids(ids)\n","print(detokenized_text_ids[:10])\n"]},{"cell_type":"markdown","id":"ac8ede12","metadata":{"papermill":{"duration":0.017275,"end_time":"2024-03-27T08:19:47.619084","exception":false,"start_time":"2024-03-27T08:19:47.601809","status":"completed"},"tags":[]},"source":["# Model Make"]},{"cell_type":"code","execution_count":37,"id":"73d34b7d","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:19:47.655723Z","iopub.status.busy":"2024-03-27T08:19:47.655389Z","iopub.status.idle":"2024-03-27T08:19:59.960172Z","shell.execute_reply":"2024-03-27T08:19:59.959083Z"},"papermill":{"duration":12.326143,"end_time":"2024-03-27T08:19:59.962573","exception":false,"start_time":"2024-03-27T08:19:47.63643","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.15.2)\r\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.20.3)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\r\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.0)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\r\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\r\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\r\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\r\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (21.3)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.1.1)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\r\n"]}],"source":["!pip install tokenizers\n"]},{"cell_type":"code","execution_count":38,"id":"03ce45af","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:20:00.003466Z","iopub.status.busy":"2024-03-27T08:20:00.003112Z","iopub.status.idle":"2024-03-27T08:20:01.10287Z","shell.execute_reply":"2024-03-27T08:20:01.101843Z"},"papermill":{"duration":1.123807,"end_time":"2024-03-27T08:20:01.105217","exception":false,"start_time":"2024-03-27T08:19:59.98141","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n"]}],"source":["from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, processors, trainers\n","\n","# Initialize the tokenizer with a BPE model\n","tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n","\n","# Normalization: Replacing spaces with a specified character\n","normalizer = normalizers.Sequence([\n","    normalizers.Replace(\" \", \"_\"),\n","    normalizers.Prepend(\"_\")\n","])\n","tokenizer.normalizer = normalizer\n","\n","# Since you mentioned \"pre_tokenizer\": null, we won't set a pre-tokenizer\n","# Note: This might not be practical for most applications\n","\n","# Post-Processor: Adding special tokens around sequences\n","tokenizer.post_processor = processors.TemplateProcessing(\n","    single=\"[CLS] $A [SEP]\",\n","    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n","    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)]\n",")\n","\n","# Assuming the \"decoder\": A custom decoding process isn't directly implemented here\n","tokenizer.decoder = decoders.Sequence([\n","    decoders.Replace(\"_\", \" \"),  # Replace spaces with underscores\n","    decoders.ByteFallback(),     # Fallback to byte encoding for unknown tokens\n","    decoders.Fuse(),             # Fuse tokens - specifics depend on implementation\n","    # Use Strip without 'start', or correctly according to its API.\n","    # This is speculative; you must replace it with the correct usage:\n","    decoders.Strip()  # Assuming Strip is to remove spaces or specific characters from tokens\n","])\n","\n","# Initialize the trainer with special tokens\n","trainer = trainers.BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n","\n","# Specify paths to your training files\n","files = [\"/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt\"]\n","\n","# Train the tokenizer\n","tokenizer.train(files, trainer)\n","\n","# Save the tokenizer for later use\n","tokenizer.save(\"custom_tokenizer.json\")\n"]},{"cell_type":"code","execution_count":39,"id":"d30f16cd","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:20:01.145299Z","iopub.status.busy":"2024-03-27T08:20:01.144322Z","iopub.status.idle":"2024-03-27T08:20:01.205413Z","shell.execute_reply":"2024-03-27T08:20:01.204395Z"},"papermill":{"duration":0.083338,"end_time":"2024-03-27T08:20:01.207502","exception":false,"start_time":"2024-03-27T08:20:01.124164","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["['[CLS]', '_Here_is_', 'some_', 't', 'ex', 't_to_', 'en', 'co', 'de', '[SEP]']\n","[1, 3855, 586, 76, 257, 1926, 100, 844, 417, 2]\n"]}],"source":["from tokenizers import Tokenizer\n","\n","# Load the tokenizer\n","tokenizer = Tokenizer.from_file(\"/kaggle/working/custom_tokenizer.json\")\n","\n","# Encode some text\n","encoded = tokenizer.encode(\"Here is some text to encode\")\n","\n","# Print the tokens and IDs\n","print(encoded.tokens)\n","print(encoded.ids)\n"]},{"cell_type":"code","execution_count":40,"id":"9b23a4a9","metadata":{"execution":{"iopub.execute_input":"2024-03-27T08:20:01.247732Z","iopub.status.busy":"2024-03-27T08:20:01.246924Z","iopub.status.idle":"2024-03-27T08:20:01.304222Z","shell.execute_reply":"2024-03-27T08:20:01.303234Z"},"papermill":{"duration":0.079413,"end_time":"2024-03-27T08:20:01.306185","exception":false,"start_time":"2024-03-27T08:20:01.226772","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":[" Here is some text to encode\n"]}],"source":["# Assuming you've already loaded your tokenizer as shown previously\n","from tokenizers import Tokenizer\n","\n","tokenizer = Tokenizer.from_file(\"/kaggle/working/custom_tokenizer.json\")\n","\n","\n","# Decode the token IDs\n","decoded_text = tokenizer.decode(encoded.ids)\n","# Replace underscores with spaces in the decoded text\n","decoded_text_with_spaces = decoded_text.replace(\"_\", \" \")\n","print(decoded_text_with_spaces)\n"]},{"cell_type":"code","execution_count":null,"id":"bb5b7acb","metadata":{"papermill":{"duration":0.017948,"end_time":"2024-03-27T08:20:01.342593","exception":false,"start_time":"2024-03-27T08:20:01.324645","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4640957,"sourceId":7902037,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":238.464876,"end_time":"2024-03-27T08:20:01.678648","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-27T08:16:03.213772","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}