{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/golammostofas/depth-knowledge-on-tokenization?scriptVersionId=168632560\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":null,"id":"af0f4434","metadata":{"_cell_guid":"8d01a664-aa16-4771-8581-b9fba3abc318","_uuid":"7a05e32e-5094-4d77-b8e2-0ddf6bd2d46d","collapsed":false,"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.013067,"end_time":"2024-03-25T04:14:27.527958","exception":false,"start_time":"2024-03-25T04:14:27.514891","status":"completed"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","id":"6148ef21","metadata":{"papermill":{"duration":0.012583,"end_time":"2024-03-25T04:14:27.553266","exception":false,"start_time":"2024-03-25T04:14:27.540683","status":"completed"},"tags":[]},"source":[]},{"cell_type":"markdown","id":"72886a4a","metadata":{"_cell_guid":"750482ac-b30a-40be-b2a0-54978addee61","_uuid":"a38c4952-764d-4b9c-8e18-52f1184b6861","papermill":{"duration":0.012269,"end_time":"2024-03-25T04:14:27.578187","exception":false,"start_time":"2024-03-25T04:14:27.565918","status":"completed"},"tags":[]},"source":["# Abstract\n","Tokenization is the process of converting a sequence of text into individual units, commonly known as ‘token’. In NLP context, tokens can represent word, subword, or even characters. \n","The primary goal is to prepare raw text data into a format that computational models can more easily analyze.\n","\n","## Role in Large Language Models(LLMs):\n","1. **Traning Phase:**  Data Preprocessing, Sequince Alignments\n","2. **Inference Phase:** Query Understanding, Output Generation"]},{"cell_type":"markdown","id":"6033ac04","metadata":{"_cell_guid":"33dbcd17-d887-4c14-bc6f-7c13c755a1b2","_uuid":"dfbabc75-bbf7-4f03-b1c3-ddb38660e108","papermill":{"duration":0.01295,"end_time":"2024-03-25T04:14:27.603723","exception":false,"start_time":"2024-03-25T04:14:27.590773","status":"completed"},"tags":[]},"source":["## Type of Tokenization\n","\n","1. Word Tokenization\n","2. Subword Tokenization\n","3. Character Tokenization\n","4. Morphological Tokenizaton"]},{"cell_type":"markdown","id":"1e822663","metadata":{"_cell_guid":"7bf9abf5-c0fa-4dac-a98f-49abe1ee80ed","_uuid":"f238c28c-799f-417f-86dd-f0ac1cca4b4f","papermill":{"duration":0.012205,"end_time":"2024-03-25T04:14:27.628913","exception":false,"start_time":"2024-03-25T04:14:27.616708","status":"completed"},"tags":[]},"source":["### 1. Word Tokenization:\n","\n","word Tokenization is one of the earliest and simplest forms of text segmentation. It generally involves splitting a sequence of text into individual word.\n","\n","2 type common algorithm:\n","1. Whitespace Tokenization\n","2. Rule-Based Tokenization"]},{"cell_type":"markdown","id":"38905eca","metadata":{"_cell_guid":"abd5f38a-d555-41b0-9ddf-269519d536ef","_uuid":"02318060-088c-4c4e-9e21-ee88d6c3c675","papermill":{"duration":0.012533,"end_time":"2024-03-25T04:14:27.653951","exception":false,"start_time":"2024-03-25T04:14:27.641418","status":"completed"},"tags":[]},"source":["#### 1.1. Whitespace Tokenization\n","The most basic from of word tokenization is whitespace tokenization, whitch splits text based on spaces."]},{"cell_type":"code","execution_count":1,"id":"456638d9","metadata":{"_cell_guid":"a660a78a-4500-4225-91e0-5bade051428c","_uuid":"a7090380-4e63-45bb-b47f-49329c078aac","collapsed":false,"execution":{"iopub.execute_input":"2024-03-25T04:14:27.682438Z","iopub.status.busy":"2024-03-25T04:14:27.681715Z","iopub.status.idle":"2024-03-25T04:14:27.696132Z","shell.execute_reply":"2024-03-25T04:14:27.694987Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.030989,"end_time":"2024-03-25T04:14:27.698075","exception":false,"start_time":"2024-03-25T04:14:27.667086","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'whitespace', 'tokenization']\n"]}],"source":["# code of whitespace tokenization\n","\n","def whitespace_tokenization(text):\n","    return text.split()\n","\n","text = 'Hello, this is an example text to demonstrate whitespace tokenization'\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"4e0e5cfb","metadata":{"_cell_guid":"e8bfcdfe-c0fc-4c1c-8a32-c024a70e66b5","_uuid":"036ce52a-b68b-4b91-b975-eb5590c57dff","papermill":{"duration":0.013412,"end_time":"2024-03-25T04:14:27.724164","exception":false,"start_time":"2024-03-25T04:14:27.710752","status":"completed"},"tags":[]},"source":["#### 1.2. Rule-Base Tokenization\n","This approach used a set of predefined rules and regex patterns to identify tokens. Example:"]},{"cell_type":"code","execution_count":2,"id":"dc3ed855","metadata":{"_cell_guid":"913c6bc5-6edb-47d7-b9ca-12c38e24e9c1","_uuid":"2567987c-2dc1-4519-8f35-f020aaccef2a","collapsed":false,"execution":{"iopub.execute_input":"2024-03-25T04:14:27.752093Z","iopub.status.busy":"2024-03-25T04:14:27.751225Z","iopub.status.idle":"2024-03-25T04:14:27.758115Z","shell.execute_reply":"2024-03-25T04:14:27.757255Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022756,"end_time":"2024-03-25T04:14:27.760173","exception":false,"start_time":"2024-03-25T04:14:27.737417","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens:  ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# code of rule-base tokenization:\n","\n","import re\n","\n","\n","def rule_base_tokenization(text):\n","    # define regex pattern\n","    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n","    \n","    tokens = re.findall(pattern, text)\n","    \n","    return tokens\n","\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens: ', tokens)"]},{"cell_type":"markdown","id":"cd15b792","metadata":{"_cell_guid":"aee8ea5d-9b2b-462e-936c-fa294a22fc7e","_uuid":"105359d5-f858-4c77-8153-f09fb47c1d11","papermill":{"duration":0.012438,"end_time":"2024-03-25T04:14:27.785269","exception":false,"start_time":"2024-03-25T04:14:27.772831","status":"completed"},"tags":[]},"source":["##### comparison between whitespace and rule-based tokenization"]},{"cell_type":"code","execution_count":3,"id":"d519dd5f","metadata":{"_cell_guid":"c1b62537-e848-449a-a1a9-89f20ff79503","_uuid":"ac750ca1-c884-44db-be87-8a0c613e8fe6","collapsed":false,"execution":{"iopub.execute_input":"2024-03-25T04:14:27.812753Z","iopub.status.busy":"2024-03-25T04:14:27.812003Z","iopub.status.idle":"2024-03-25T04:14:27.817564Z","shell.execute_reply":"2024-03-25T04:14:27.816567Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022363,"end_time":"2024-03-25T04:14:27.820355","exception":false,"start_time":"2024-03-25T04:14:27.797992","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokens by Whitespace Tokenization:\n"," ['Hello,', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule-based', 'tokenization!', \"Isn't\", 'it', 'great?']\n","Tokens by Rule-base Tokenization:\n"," ['Hello', ',', 'this', 'is', 'an', 'example', 'text', 'to', 'demonstrate', 'rule', '-', 'based', 'tokenization', '!', \"Isn't\", 'it', 'great', '?']\n"]}],"source":["# used common text\n","text = f'''Hello, this is an example text to demonstrate rule-based tokenization! Isn't it great?'''\n","\n","tokens = whitespace_tokenization(text)\n","print('Tokens by Whitespace Tokenization:\\n', tokens)\n","\n","tokens = rule_base_tokenization(text)\n","print('Tokens by Rule-base Tokenization:\\n', tokens)"]},{"cell_type":"markdown","id":"3ed554ad","metadata":{"_cell_guid":"74110206-112e-4349-8901-f9660fdcf05f","_uuid":"e6d0409d-ddc1-4139-bfeb-dd3490fb38b3","papermill":{"duration":0.01249,"end_time":"2024-03-25T04:14:27.845548","exception":false,"start_time":"2024-03-25T04:14:27.833058","status":"completed"},"tags":[]},"source":["### 2. Subword Tokenization\n","\n","Subword Tokenization techniques operate at a lavel between words and characterss, aiming to capture meaningful linguistic units smaller then a word but large then a character\n","\n","3 common algorithms:\n","\n","1. Byte Pair Encoding(BPE)\n","2. WordPiece \n","3. SentencePiece"]},{"cell_type":"markdown","id":"acaf4853","metadata":{"_cell_guid":"5ea3227f-8222-4378-9ab9-3fe05da4ea70","_uuid":"89acf4dc-319f-49d8-a4f2-fdd9d510784d","papermill":{"duration":0.012408,"end_time":"2024-03-25T04:14:27.870821","exception":false,"start_time":"2024-03-25T04:14:27.858413","status":"completed"},"tags":[]},"source":["#### 2.1. Byte Pair Encoding(BPE)\n","\n","BPE works by iteratively merginf the most frequently occurring character or character sequences. Following a somplified illustration of how BPE works tokenizing text:\n","* **Initialization:** Start with individual characters or symbols as the basic tokens\n","* **Frequency Count:** Count all adjancent pairs of tokens in the dataset\n","* **Merge:** Identifiy the most frequent pair of tokens and merge them into a single new token\n","* **Repeat:** Repeat the frequency count and merge steps untill a sepecified number of merges has been reached or the vocabulary reaches a desired size\n","\n","Example:\n","\n","Suppose the data to be encoded is\n","> aaabdaaabac\n","\n","The byte pair \"aa\" occurs most often, so it will be replaced by a byte that is not used in the data, such as \"Z\". Now there is the following data and replacement table:\n","> ZabdZabac\n","\n","> Z=aa\n","\n","Then the process is repeated with byte pair \"ab\", replacing it with \"Y\":\n","> ZYdZYac\n","\n","> Y=ab\n","\n","> Z=aa\n","\n","The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing \"ZY\" with \"X\":\n","> XdXac\n","\n","> X=ZY\n","\n","> Y=ab\n","\n","> Z=aa\n","\n","*This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.*\n","\n","**N.B:** To decompress the data, simply perform the replacements in the reverse order.\n","Below is a basic python implementation of BPE:"]},{"cell_type":"code","execution_count":4,"id":"3e2fdc5b","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:27.898341Z","iopub.status.busy":"2024-03-25T04:14:27.897501Z","iopub.status.idle":"2024-03-25T04:14:27.906647Z","shell.execute_reply":"2024-03-25T04:14:27.905752Z"},"papermill":{"duration":0.025065,"end_time":"2024-03-25T04:14:27.908709","exception":false,"start_time":"2024-03-25T04:14:27.883644","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'this': 1, 'is': 1, 'an': 2, 'example.': 1, 'I': 1, 'am': 1, 'engineer': 1}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from collections import defaultdict, Counter\n","\n","def get_vocab(text):\n","    \"\"\"Split text into symbles and connt symbols frequencies\"\"\"\n","    \n","    vocab = Counter(text.split())\n","    \n","    #convert vocabulary to format {'word': count}\n","    \n","    return {word: freq for word, freq in vocab.items()}\n","\n","get_vocab('this is an example. I am an engineer')\n","    "]},{"cell_type":"code","execution_count":5,"id":"c8b91f5b","metadata":{"_cell_guid":"894656c7-c18c-40be-b1cf-4abf83b43c4f","_uuid":"7b3b553c-5c7a-429e-81b8-30a7fda47819","collapsed":false,"execution":{"iopub.execute_input":"2024-03-25T04:14:27.936871Z","iopub.status.busy":"2024-03-25T04:14:27.936432Z","iopub.status.idle":"2024-03-25T04:14:27.945496Z","shell.execute_reply":"2024-03-25T04:14:27.94446Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.025688,"end_time":"2024-03-25T04:14:27.947756","exception":false,"start_time":"2024-03-25T04:14:27.922068","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["defaultdict(int, {})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\n","def get_stats(vocab):\n","    \n","    pairs = defaultdict(int)\n","\n","    for word, freq in vocab.items():\n","        symbols = word.split()\n","        for i  in range(len(symbols) - 1):\n","            pairs[symbols[i], symbols[i+1]] += freq\n","    return pairs\n","\n","vocab = get_vocab('this is an example. I am an engineer.')\n","get_stats(vocab)"]},{"cell_type":"code","execution_count":6,"id":"28004c99","metadata":{"_cell_guid":"79b8a12c-42e0-4db7-a701-a1a8eb1d3d65","_uuid":"430c4da5-4e20-47f6-aa81-ebcf754b7dc7","collapsed":false,"execution":{"iopub.execute_input":"2024-03-25T04:14:27.976962Z","iopub.status.busy":"2024-03-25T04:14:27.97659Z","iopub.status.idle":"2024-03-25T04:14:27.982392Z","shell.execute_reply":"2024-03-25T04:14:27.981379Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":0.022943,"end_time":"2024-03-25T04:14:27.984639","exception":false,"start_time":"2024-03-25T04:14:27.961696","status":"completed"},"tags":[]},"outputs":[],"source":["def marge_vocab(pair, vocab):\n","    new_vocab = {}\n","    bigram = ' '.join(pair)\n","    replacement = ''.join(pair)\n","    for word in vocab:\n","        new_word = word.replace(bigram, replacement)\n","        new_vocab[new_word] = vocab[word]\n","        \n","    return new_vocab\n","\n"]},{"cell_type":"code","execution_count":7,"id":"f6cd9192","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.013624Z","iopub.status.busy":"2024-03-25T04:14:28.013222Z","iopub.status.idle":"2024-03-25T04:14:28.023199Z","shell.execute_reply":"2024-03-25T04:14:28.022266Z"},"papermill":{"duration":0.026968,"end_time":"2024-03-25T04:14:28.025321","exception":false,"start_time":"2024-03-25T04:14:27.998353","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["{'I', 'am', 'an', 'engineer.', 'example.', 'is', 'test', 'this'}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["def bpe_tokenize(text, number_merges):\n","    vocab = get_vocab(text)\n","    for i in range(number_merges):\n","        pairs = get_stats(vocab)\n","        if not pairs:\n","            break\n","        best = max(pairs, key=pairs.get)\n","        vocab = marge_vocab(best, vocab)\n","        \n","    tokens = set()\n","    for word in vocab:\n","        tokens.update(word.split())\n","        \n","    return tokens\n","\n","text = 'this is an example. I am an engineer. this is test'\n","\n","tokens = bpe_tokenize(text, 10)\n","tokens"]},{"cell_type":"markdown","id":"d5d0c742","metadata":{"papermill":{"duration":0.013717,"end_time":"2024-03-25T04:14:28.0531","exception":false,"start_time":"2024-03-25T04:14:28.039383","status":"completed"},"tags":[]},"source":["##### For LLMs"]},{"cell_type":"code","execution_count":8,"id":"c0df1369","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.081192Z","iopub.status.busy":"2024-03-25T04:14:28.080845Z","iopub.status.idle":"2024-03-25T04:14:28.0875Z","shell.execute_reply":"2024-03-25T04:14:28.086549Z"},"papermill":{"duration":0.023372,"end_time":"2024-03-25T04:14:28.089798","exception":false,"start_time":"2024-03-25T04:14:28.066426","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[116, 104, 105, 115, 32, 105, 115, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 73, 32, 97, 109, 32, 97, 110, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46, 32, 116, 104, 105, 115, 32, 105, 115, 32, 116, 101, 115, 116]\n","\n","len of text 50\n","len of token 50\n"]}],"source":["def encoding(text):\n","    tokens = text.encode('utf-8') # Raw bytes\n","    return list(map(int, tokens))\n","\n","text = 'this is an example. I am an engineer. this is test'\n","tokens = encoding(text)\n","print(tokens)\n","print('\\nlen of text', len(text))\n","print('len of token', len(tokens))\n","\n"]},{"cell_type":"code","execution_count":9,"id":"7fb059c7","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.118339Z","iopub.status.busy":"2024-03-25T04:14:28.117963Z","iopub.status.idle":"2024-03-25T04:14:28.12407Z","shell.execute_reply":"2024-03-25T04:14:28.123062Z"},"papermill":{"duration":0.023397,"end_time":"2024-03-25T04:14:28.126599","exception":false,"start_time":"2024-03-25T04:14:28.103202","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{(116, 104): 2, (104, 105): 2, (105, 115): 4, (115, 32): 4, (32, 105): 2, (32, 97): 3, (97, 110): 2, (110, 32): 2, (32, 101): 2, (101, 120): 1, (120, 97): 1, (97, 109): 2, (109, 112): 1, (112, 108): 1, (108, 101): 1, (101, 46): 1, (46, 32): 2, (32, 73): 1, (73, 32): 1, (109, 32): 1, (101, 110): 1, (110, 103): 1, (103, 105): 1, (105, 110): 1, (110, 101): 1, (101, 101): 1, (101, 114): 1, (114, 46): 1, (32, 116): 2, (116, 101): 1, (101, 115): 1, (115, 116): 1}\n"]}],"source":["def get_stats(ids):\n","    counts = {}\n","    for pair in zip(ids, ids[1:]):\n","        counts[pair] = counts.get(pair, 0) + 1\n","    return counts\n","\n","stats = get_stats(tokens)\n","\n","print(stats)"]},{"cell_type":"code","execution_count":10,"id":"ef25ff65","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.156546Z","iopub.status.busy":"2024-03-25T04:14:28.155854Z","iopub.status.idle":"2024-03-25T04:14:28.16253Z","shell.execute_reply":"2024-03-25T04:14:28.161551Z"},"papermill":{"duration":0.024381,"end_time":"2024-03-25T04:14:28.164819","exception":false,"start_time":"2024-03-25T04:14:28.140438","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(105, 115)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["top_pair = max(stats, key=stats.get)\n","top_pair"]},{"cell_type":"code","execution_count":11,"id":"6dd535b4","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.194222Z","iopub.status.busy":"2024-03-25T04:14:28.193873Z","iopub.status.idle":"2024-03-25T04:14:28.20008Z","shell.execute_reply":"2024-03-25T04:14:28.199161Z"},"papermill":{"duration":0.023365,"end_time":"2024-03-25T04:14:28.202128","exception":false,"start_time":"2024-03-25T04:14:28.178763","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["('i', 's')"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["chr(105), chr(115)"]},{"cell_type":"code","execution_count":12,"id":"6982e55e","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.232075Z","iopub.status.busy":"2024-03-25T04:14:28.231232Z","iopub.status.idle":"2024-03-25T04:14:28.240821Z","shell.execute_reply":"2024-03-25T04:14:28.239751Z"},"papermill":{"duration":0.026807,"end_time":"2024-03-25T04:14:28.242977","exception":false,"start_time":"2024-03-25T04:14:28.21617","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[5, 6, 99, 9, 1]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["def merge(ids, pair, idx):\n","    new_ids = []\n","    \n","    i = 0\n","    while i < len(ids):\n","        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n","            new_ids.append(idx)\n","            i += 2\n","        else:\n","            new_ids.append(ids[i])\n","            i += 1\n","    return new_ids\n","\n","merge([5, 6, 6, 7, 9, 1], (6, 7), 99)"]},{"cell_type":"code","execution_count":13,"id":"4fb43752","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.272943Z","iopub.status.busy":"2024-03-25T04:14:28.272486Z","iopub.status.idle":"2024-03-25T04:14:28.278348Z","shell.execute_reply":"2024-03-25T04:14:28.277378Z"},"papermill":{"duration":0.02283,"end_time":"2024-03-25T04:14:28.280421","exception":false,"start_time":"2024-03-25T04:14:28.257591","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[116, 104, 256, 32, 256, 32, 97, 110, 32, 101, 120, 97, 109, 112, 108, 101, 46, 32, 73, 32, 97, 109, 32, 97, 110, 32, 101, 110, 103, 105, 110, 101, 101, 114, 46, 32, 116, 104, 256, 32, 256, 32, 116, 101, 115, 116]\n","\n","len of token2: 46\n"]}],"source":["tokens2 = merge(tokens, top_pair, 256)\n","print(tokens2)\n","\n","print('\\nlen of token2:', len(tokens2))\n"]},{"cell_type":"markdown","id":"d7323057","metadata":{"papermill":{"duration":0.01345,"end_time":"2024-03-25T04:14:28.307822","exception":false,"start_time":"2024-03-25T04:14:28.294372","status":"completed"},"tags":[]},"source":["##### Make merge dataset"]},{"cell_type":"code","execution_count":14,"id":"f842d534","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:14:28.337067Z","iopub.status.busy":"2024-03-25T04:14:28.336704Z","iopub.status.idle":"2024-03-25T04:18:15.14039Z","shell.execute_reply":"2024-03-25T04:18:15.139265Z"},"papermill":{"duration":226.821435,"end_time":"2024-03-25T04:18:15.143267","exception":false,"start_time":"2024-03-25T04:14:28.321832","status":"completed"},"tags":[]},"outputs":[],"source":["#using constant time\n","vocab_size = 5000\n","number_megres = vocab_size - 256\n","\n","with open('/kaggle/input/romeo-and-juliet-tokenization/romeo-and-juliet_tokenization.txt', 'r') as file:\n","    text = file.read()\n","    \n","tokens = encoding(text)\n","ids = tokens\n","\n","merges = {}\n","\n","for i in range(number_megres):\n","    stats = get_stats(ids)\n","    \n","    pair = max(stats, key=stats.get)\n","    if stats[pair] == 1:\n","        break\n","    idx = 256 + i\n","    #print(f\"Merging: {pair} in a new token {idx}\")\n","    ids = merge(ids, pair, idx)\n","    merges[pair] = idx"]},{"cell_type":"markdown","id":"c2051297","metadata":{"papermill":{"duration":0.01411,"end_time":"2024-03-25T04:18:15.172075","exception":false,"start_time":"2024-03-25T04:18:15.157965","status":"completed"},"tags":[]},"source":["##### Evulation"]},{"cell_type":"code","execution_count":15,"id":"666165b1","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.260432Z","iopub.status.busy":"2024-03-25T04:18:15.259471Z","iopub.status.idle":"2024-03-25T04:18:15.270063Z","shell.execute_reply":"2024-03-25T04:18:15.269075Z"},"papermill":{"duration":0.030543,"end_time":"2024-03-25T04:18:15.273291","exception":false,"start_time":"2024-03-25T04:18:15.242748","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["token len 141695\n","ids len 31546\n","compression ratio:  4.4916946681037215\n"]}],"source":["print('token len', len(tokens))\n","print('ids len', len(ids))\n","print('compression ratio: ', len(tokens)/len(ids))"]},{"cell_type":"markdown","id":"ff34f3bb","metadata":{"papermill":{"duration":0.014096,"end_time":"2024-03-25T04:18:15.302462","exception":false,"start_time":"2024-03-25T04:18:15.288366","status":"completed"},"tags":[]},"source":["##### Merges download dataset"]},{"cell_type":"code","execution_count":16,"id":"03967dc0","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.333998Z","iopub.status.busy":"2024-03-25T04:18:15.333064Z","iopub.status.idle":"2024-03-25T04:18:15.341818Z","shell.execute_reply":"2024-03-25T04:18:15.340951Z"},"papermill":{"duration":0.026956,"end_time":"2024-03-25T04:18:15.344028","exception":false,"start_time":"2024-03-25T04:18:15.317072","status":"completed"},"tags":[]},"outputs":[],"source":["with open('merges.txt', 'w') as file:\n","    file.write(str(merges))"]},{"cell_type":"markdown","id":"e9340163","metadata":{"papermill":{"duration":0.014287,"end_time":"2024-03-25T04:18:15.373322","exception":false,"start_time":"2024-03-25T04:18:15.359035","status":"completed"},"tags":[]},"source":["##### load merges data"]},{"cell_type":"code","execution_count":17,"id":"ddf8126a","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.403757Z","iopub.status.busy":"2024-03-25T04:18:15.402858Z","iopub.status.idle":"2024-03-25T04:18:15.522734Z","shell.execute_reply":"2024-03-25T04:18:15.521861Z"},"papermill":{"duration":0.137805,"end_time":"2024-03-25T04:18:15.525291","exception":false,"start_time":"2024-03-25T04:18:15.387486","status":"completed"},"tags":[]},"outputs":[],"source":["import ast\n","\n","with open('merges.txt', 'r') as file:\n","    data_string = file.read()\n","    merges = ast.literal_eval(data_string)\n"]},{"cell_type":"markdown","id":"af78560d","metadata":{"papermill":{"duration":0.014455,"end_time":"2024-03-25T04:18:15.554363","exception":false,"start_time":"2024-03-25T04:18:15.539908","status":"completed"},"tags":[]},"source":["##### Encoding"]},{"cell_type":"code","execution_count":18,"id":"7cbb2861","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.585335Z","iopub.status.busy":"2024-03-25T04:18:15.584465Z","iopub.status.idle":"2024-03-25T04:18:15.592917Z","shell.execute_reply":"2024-03-25T04:18:15.592036Z"},"papermill":{"duration":0.026298,"end_time":"2024-03-25T04:18:15.59517","exception":false,"start_time":"2024-03-25T04:18:15.568872","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["[97]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["def encode(text):\n","    tokens = list(text.encode('utf-8'))\n","    \n","    while len(tokens)>=2:\n","        stats = get_stats(tokens)\n","        pair = min(stats, key=lambda p: merges.get(p, float('inf')))\n","        \n","        if pair not in merges:\n","            break\n","        \n","        idx = merges[pair]\n","        tokens = merge(tokens, pair, idx)\n","        \n","        \n","    return tokens\n","\n","encode('a')"]},{"cell_type":"markdown","id":"c64a43c3","metadata":{"papermill":{"duration":0.015037,"end_time":"2024-03-25T04:18:15.624678","exception":false,"start_time":"2024-03-25T04:18:15.609641","status":"completed"},"tags":[]},"source":["##### Vocab download"]},{"cell_type":"code","execution_count":19,"id":"e4b7676d","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.655009Z","iopub.status.busy":"2024-03-25T04:18:15.654323Z","iopub.status.idle":"2024-03-25T04:18:15.665018Z","shell.execute_reply":"2024-03-25T04:18:15.664014Z"},"papermill":{"duration":0.028146,"end_time":"2024-03-25T04:18:15.667114","exception":false,"start_time":"2024-03-25T04:18:15.638968","status":"completed"},"tags":[]},"outputs":[],"source":["vocab = {idx: bytes([idx]) for idx in range(256)}\n","\n","for (p0, p1), idx in merges.items():\n","    vocab[idx] = vocab[p0] + vocab[p1]\n","    \n","with open('vocab.txt', 'w') as file:\n","    file.write(str(vocab))\n"]},{"cell_type":"markdown","id":"676ca4a4","metadata":{"papermill":{"duration":0.014108,"end_time":"2024-03-25T04:18:15.695337","exception":false,"start_time":"2024-03-25T04:18:15.681229","status":"completed"},"tags":[]},"source":["##### Load Vocab"]},{"cell_type":"code","execution_count":20,"id":"c2f99bdb","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.726052Z","iopub.status.busy":"2024-03-25T04:18:15.725338Z","iopub.status.idle":"2024-03-25T04:18:15.756665Z","shell.execute_reply":"2024-03-25T04:18:15.755796Z"},"papermill":{"duration":0.049781,"end_time":"2024-03-25T04:18:15.759315","exception":false,"start_time":"2024-03-25T04:18:15.709534","status":"completed"},"tags":[]},"outputs":[],"source":["import ast\n","\n","with open('vocab.txt', 'r') as file:\n","    data_string = file.read()\n","    vocab = ast.literal_eval(data_string)\n","\n","\n"]},{"cell_type":"markdown","id":"874f953e","metadata":{"papermill":{"duration":0.014162,"end_time":"2024-03-25T04:18:15.78822","exception":false,"start_time":"2024-03-25T04:18:15.774058","status":"completed"},"tags":[]},"source":["##### Decoding"]},{"cell_type":"code","execution_count":21,"id":"c58c0067","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.819399Z","iopub.status.busy":"2024-03-25T04:18:15.818656Z","iopub.status.idle":"2024-03-25T04:18:15.82462Z","shell.execute_reply":"2024-03-25T04:18:15.823585Z"},"papermill":{"duration":0.025381,"end_time":"2024-03-25T04:18:15.828062","exception":false,"start_time":"2024-03-25T04:18:15.802681","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["a\n"]}],"source":["\n","def decode(ids):\n","    tokens = b''.join(vocab[idx] for idx in ids)\n","    text = tokens.decode('utf-8', errors='replace')\n","    \n","    return text\n","\n","\n","print(decode([97]))"]},{"cell_type":"code","execution_count":22,"id":"951c07c4","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.860743Z","iopub.status.busy":"2024-03-25T04:18:15.860257Z","iopub.status.idle":"2024-03-25T04:18:15.866859Z","shell.execute_reply":"2024-03-25T04:18:15.865892Z"},"papermill":{"duration":0.02559,"end_time":"2024-03-25T04:18:15.869068","exception":false,"start_time":"2024-03-25T04:18:15.843478","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["'Hello World!'"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["decode(encode('Hello World!'))"]},{"cell_type":"markdown","id":"48478ca6","metadata":{"papermill":{"duration":0.014451,"end_time":"2024-03-25T04:18:15.898478","exception":false,"start_time":"2024-03-25T04:18:15.884027","status":"completed"},"tags":[]},"source":["#### 2.2. WordPiece\n","\n","Word Piece Tokenization is a method used primarily in natural language processing (NLP) tasks to split text into manageable pieces or tokens. This technique is especially beneficial for dealing with languages that have a rich morphology or for processing text that includes a lot of unfamiliar words or names. The core idea behind Word Piece Tokenization is to balance between the granularity of character-level tokenization and the broader scope of word-level tokenization, effectively improving model performance in various NLP tasks.\n","\n","\n","Tokenization Process:\n","\n","1. Starting Point: Words are first checked against the vocabulary. If a word is found, it is used as a token as is.\n","\n","2. Subword Segmentation: For words not found in the vocabulary, the algorithm attempts to break the word into the largest possible subwords available in the vocabulary. This step is iterated until the whole word is segmented into known subwords or characters.\n","\n","3. Rare Words Handling: The inclusion of characters in the vocabulary ensures that any word, no matter how uncommon or complex, can be tokenized into familiar units, allowing the model to attempt understanding and processing it.\n","\n","\n","Let's consider the word \"unhappiness\" to illustrate the process of WordPiece Tokenization with a more complex example. We'll assume our model's vocabulary includes the following tokens, among others:\n","\n","un\n","##happy\n","##ness\n","happiness\n","Given the word \"unhappiness\", here's how WordPiece Tokenization would proceed:\n","\n","Full Word Matching: The tokenizer first checks if \"unhappiness\" is present as a complete word in its vocabulary. If \"unhappiness\" isn't found as a whole word, the tokenizer moves to break down the word into known subwords or tokens.\n","\n","Subword Segmentation:\n","\n","The tokenizer identifies the prefix \"un\" as a common prefix in its vocabulary, indicating negation or the opposite of the base word.\n","Next, it looks for the longest possible matching sequence that follows \"un\". While \"happiness\" might be in the vocabulary, the tokenizer efficiently breaks down complex words into smaller subwords to maximize vocabulary usage and handle variations robustly.\n","After \"un\", it might identify \"happy\" as a base word. However, since \"happy\" is not standalone in this context, the tokenizer uses the token ##happy to indicate that \"happy\" is part of a larger word.\n","Finally, it recognizes the suffix \"ness\", which is represented in the vocabulary as ##ness to indicate that it's attached to another token.\n","Tokenization Output: Combining these tokens, the process results in the following sequence:\n","\n","un, ##happy, ##ness\n"]},{"cell_type":"code","execution_count":23,"id":"4c808739","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.930262Z","iopub.status.busy":"2024-03-25T04:18:15.929871Z","iopub.status.idle":"2024-03-25T04:18:15.937614Z","shell.execute_reply":"2024-03-25T04:18:15.936543Z"},"papermill":{"duration":0.025948,"end_time":"2024-03-25T04:18:15.939725","exception":false,"start_time":"2024-03-25T04:18:15.913777","status":"completed"},"tags":[]},"outputs":[],"source":["corpus = [\n","    \"WordPiece tokenization is awesome\",\n","    \"Language models use WordPiece\",\n","    \"The tokenization process is interesting\"\n","]\n","\n","# Tokenize the corpus into words (naive approach for simplicity)\n","words = [word.lower() for sentence in corpus for word in sentence.split()]\n","\n","# Further break down into subwords/characters (naive splitting for demonstration)\n","subwords = set()\n","for word in words:\n","    length = len(word)\n","    for i in range(length):\n","        for j in range(i+1, length+1):\n","            subwords.add(word[i:j])\n","            "]},{"cell_type":"code","execution_count":24,"id":"fe44fea8","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:15.972494Z","iopub.status.busy":"2024-03-25T04:18:15.971663Z","iopub.status.idle":"2024-03-25T04:18:15.978682Z","shell.execute_reply":"2024-03-25T04:18:15.977726Z"},"papermill":{"duration":0.025718,"end_time":"2024-03-25T04:18:15.980741","exception":false,"start_time":"2024-03-25T04:18:15.955023","status":"completed"},"tags":[]},"outputs":[],"source":["vocab = {idx: chr(idx) for idx in range(256)}\n","\n","for i, words in enumerate(subwords):\n","    if words not in vocab.items():\n","        vocab[256 + i] = words\n","        \n","with open('word piece vocab.txt', 'w') as file:\n","    file.write(str(vocab))"]},{"cell_type":"code","execution_count":25,"id":"b302c506","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:16.011391Z","iopub.status.busy":"2024-03-25T04:18:16.010664Z","iopub.status.idle":"2024-03-25T04:18:16.018264Z","shell.execute_reply":"2024-03-25T04:18:16.017371Z"},"papermill":{"duration":0.025309,"end_time":"2024-03-25T04:18:16.02063","exception":false,"start_time":"2024-03-25T04:18:15.995321","status":"completed"},"tags":[]},"outputs":[],"source":["# Load Vocab\n","with open('word piece vocab.txt', 'r', encoding='utf-8') as file:\n","    data_string = file.read()\n","    vocab = ast.literal_eval(data_string)"]},{"cell_type":"code","execution_count":26,"id":"ef29136c","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:16.051813Z","iopub.status.busy":"2024-03-25T04:18:16.051442Z","iopub.status.idle":"2024-03-25T04:18:16.059019Z","shell.execute_reply":"2024-03-25T04:18:16.058039Z"},"papermill":{"duration":0.025429,"end_time":"2024-03-25T04:18:16.061109","exception":false,"start_time":"2024-03-25T04:18:16.03568","status":"completed"},"tags":[]},"outputs":[],"source":["\n","def encode_text(text, vocab):\n","    # Reverse the vocab to map items to their indexes for encoding\n","    item_to_index = {v: k for k, v in vocab.items()}\n","    \n","    encoded_ids = []\n","    for word in text.split():\n","        encoded_word = False\n","        for i in range(len(word), 0, -1):\n","            if word[:i] in item_to_index.keys():\n","                encoded_ids.append(item_to_index[word[:i]])\n","                word = word[i:]\n","        if not encoded_word:  # If part of the word wasn't in the vocab\n","            for char in word:  # Fallback to character encoding\n","                if char in item_to_index:\n","                    encoded_ids.append(item_to_index[char])\n","    return encoded_ids\n"]},{"cell_type":"code","execution_count":27,"id":"5b2eb2b3","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:16.092198Z","iopub.status.busy":"2024-03-25T04:18:16.091479Z","iopub.status.idle":"2024-03-25T04:18:16.096448Z","shell.execute_reply":"2024-03-25T04:18:16.095537Z"},"papermill":{"duration":0.022736,"end_time":"2024-03-25T04:18:16.098413","exception":false,"start_time":"2024-03-25T04:18:16.075677","status":"completed"},"tags":[]},"outputs":[],"source":["# Decoder\n","def decode_ids(encoded_ids, vocab):\n","    decoded_text = ''\n","    for id in encoded_ids:\n","        decoded_text += vocab[id]\n","    return decoded_text\n"]},{"cell_type":"code","execution_count":28,"id":"0a62b7d7","metadata":{"execution":{"iopub.execute_input":"2024-03-25T04:18:16.129095Z","iopub.status.busy":"2024-03-25T04:18:16.128757Z","iopub.status.idle":"2024-03-25T04:18:16.134274Z","shell.execute_reply":"2024-03-25T04:18:16.133357Z"},"papermill":{"duration":0.023776,"end_time":"2024-03-25T04:18:16.136882","exception":false,"start_time":"2024-03-25T04:18:16.113106","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoded Tokens: [87, 515, 416, 356, 80, 405, 325, 420, 325, 398, 502, 287, 373, 438, 294]\n","decoded_text: WordPiecehelpsintokenization\n"]}],"source":["text2 = \"WordPiece helps in tokenization\"\n","\n","encoded_tokens = encode_text(text2, vocab)\n","decoded_text = decode_ids(encoded_tokens, vocab)\n","\n","print(\"Encoded Tokens:\", encoded_tokens)\n","print(\"decoded_text:\", decoded_text)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4640957,"sourceId":7902037,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":232.233684,"end_time":"2024-03-25T04:18:16.471038","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-25T04:14:24.237354","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}